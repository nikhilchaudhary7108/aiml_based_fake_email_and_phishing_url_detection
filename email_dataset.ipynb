{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973a14af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\wcukierski\\enron-email-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ad17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\wcukierski\\enron-email-dataset\\versions\\2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ba5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a76768",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_dataset = os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35930e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails.csv\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for file in files_in_dataset:\n",
    "      print(file)\n",
    "      print('*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f842288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c650089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataset_path,\"emails.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9236352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>zufferli-j/sent_items/95.</td>\n",
       "      <td>Message-ID: &lt;26807948.1075842029936.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>zufferli-j/sent_items/96.</td>\n",
       "      <td>Message-ID: &lt;25835861.1075842029959.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517398</th>\n",
       "      <td>zufferli-j/sent_items/97.</td>\n",
       "      <td>Message-ID: &lt;28979867.1075842029988.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>zufferli-j/sent_items/98.</td>\n",
       "      <td>Message-ID: &lt;22052556.1075842030013.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517400</th>\n",
       "      <td>zufferli-j/sent_items/99.</td>\n",
       "      <td>Message-ID: &lt;28618979.1075842030037.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517401 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  \\\n",
       "0           allen-p/_sent_mail/1.   \n",
       "1          allen-p/_sent_mail/10.   \n",
       "2         allen-p/_sent_mail/100.   \n",
       "3        allen-p/_sent_mail/1000.   \n",
       "4        allen-p/_sent_mail/1001.   \n",
       "...                           ...   \n",
       "517396  zufferli-j/sent_items/95.   \n",
       "517397  zufferli-j/sent_items/96.   \n",
       "517398  zufferli-j/sent_items/97.   \n",
       "517399  zufferli-j/sent_items/98.   \n",
       "517400  zufferli-j/sent_items/99.   \n",
       "\n",
       "                                                  message  \n",
       "0       Message-ID: <18782981.1075855378110.JavaMail.e...  \n",
       "1       Message-ID: <15464986.1075855378456.JavaMail.e...  \n",
       "2       Message-ID: <24216240.1075855687451.JavaMail.e...  \n",
       "3       Message-ID: <13505866.1075863688222.JavaMail.e...  \n",
       "4       Message-ID: <30922949.1075863688243.JavaMail.e...  \n",
       "...                                                   ...  \n",
       "517396  Message-ID: <26807948.1075842029936.JavaMail.e...  \n",
       "517397  Message-ID: <25835861.1075842029959.JavaMail.e...  \n",
       "517398  Message-ID: <28979867.1075842029988.JavaMail.e...  \n",
       "517399  Message-ID: <22052556.1075842030013.JavaMail.e...  \n",
       "517400  Message-ID: <28618979.1075842030037.JavaMail.e...  \n",
       "\n",
       "[517401 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da18faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c3d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    parts = text.split(\"\\n\\n\", 1)   # split into header + body\n",
    "    return parts[1] if len(parts) > 1 else text\n",
    "\n",
    "df['message'] = df['message'].apply(extract_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "562a4dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>This is a trade with OIL-SPEC-HEDGE-NG (John L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>Some of my position is with the Alberta Term b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517398</th>\n",
       "      <td>2\\n\\n -----Original Message-----\\nFrom: \\tDouc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>Analyst\\t\\t\\t\\t\\tRank\\n\\nStephane Brodeur\\t\\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517400</th>\n",
       "      <td>i think the YMCA has a class that is for peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517401 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message\n",
       "0                               Here is our forecast\\n\\n \n",
       "1       Traveling to have a business meeting takes the...\n",
       "2                          test successful.  way to go!!!\n",
       "3       Randy,\\n\\n Can you send me a schedule of the s...\n",
       "4                     Let's shoot for Tuesday at 11:45.  \n",
       "...                                                   ...\n",
       "517396  This is a trade with OIL-SPEC-HEDGE-NG (John L...\n",
       "517397  Some of my position is with the Alberta Term b...\n",
       "517398  2\\n\\n -----Original Message-----\\nFrom: \\tDouc...\n",
       "517399  Analyst\\t\\t\\t\\t\\tRank\\n\\nStephane Brodeur\\t\\t\\...\n",
       "517400  i think the YMCA has a class that is for peopl...\n",
       "\n",
       "[517401 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0660a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_message(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)          # collapse whitespace\n",
    "    text = re.sub(r'(?i)(forwarded by|original message|from:.*@)', '', text)  # basic header artifacts\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['message'] = df['message'].apply(clean_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e8bc123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy, Can you send me a schedule of the salar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>This is a trade with OIL-SPEC-HEDGE-NG (John L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>Some of my position is with the Alberta Term b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517398</th>\n",
       "      <td>2 ---------- From: Doucet, Dawn Sent: Wednesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517400</th>\n",
       "      <td>i think the YMCA has a class that is for peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517401 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message\n",
       "0                                    Here is our forecast\n",
       "1       Traveling to have a business meeting takes the...\n",
       "2                           test successful. way to go!!!\n",
       "3       Randy, Can you send me a schedule of the salar...\n",
       "4                       Let's shoot for Tuesday at 11:45.\n",
       "...                                                   ...\n",
       "517396  This is a trade with OIL-SPEC-HEDGE-NG (John L...\n",
       "517397  Some of my position is with the Alberta Term b...\n",
       "517398  2 ---------- From: Doucet, Dawn Sent: Wednesda...\n",
       "517399  Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...\n",
       "517400  i think the YMCA has a class that is for peopl...\n",
       "\n",
       "[517401 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d3ae372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "905797c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(274962)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e542e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message\n",
      "0                               Here is our forecast\n",
      "1  Traveling to have a business meeting takes the...\n",
      "2                      test successful. way to go!!!\n",
      "3  Randy, Can you send me a schedule of the salar...\n",
      "4                  Let's shoot for Tuesday at 11:45.\n",
      "Total duplicates (including all copies): 406522\n"
     ]
    }
   ],
   "source": [
    "dupes = df[df.duplicated(keep=False)]\n",
    "print(dupes.head())\n",
    "print(\"Total duplicates (including all copies):\", len(dupes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10474920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Let me know when you get the quotes from Pauli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Tim, mike grigsby is having problems with acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Reagan, Just wanted to give you an update. I h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               message\n",
       "277                               Here is our forecast\n",
       "279  Let me know when you get the quotes from Pauli...\n",
       "286  Traveling to have a business meeting takes the...\n",
       "287  Tim, mike grigsby is having problems with acce...\n",
       "288  Reagan, Just wanted to give you an update. I h..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupes = df[df.duplicated(keep='first')].head()\n",
    "dupes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e82dfecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy, Can you send me a schedule of the salar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>This is a trade with OIL-SPEC-HEDGE-NG (John L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>Some of my position is with the Alberta Term b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517398</th>\n",
       "      <td>2 ---------- From: Doucet, Dawn Sent: Wednesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517400</th>\n",
       "      <td>i think the YMCA has a class that is for peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517401 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message\n",
       "0                                    Here is our forecast\n",
       "1       Traveling to have a business meeting takes the...\n",
       "2                           test successful. way to go!!!\n",
       "3       Randy, Can you send me a schedule of the salar...\n",
       "4                       Let's shoot for Tuesday at 11:45.\n",
       "...                                                   ...\n",
       "517396  This is a trade with OIL-SPEC-HEDGE-NG (John L...\n",
       "517397  Some of my position is with the Alberta Term b...\n",
       "517398  2 ---------- From: Doucet, Dawn Sent: Wednesda...\n",
       "517399  Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...\n",
       "517400  i think the YMCA has a class that is for peopl...\n",
       "\n",
       "[517401 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "febd98f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicates: (242439, 1)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(\"After removing duplicates:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cab8e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3ac08db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy, Can you send me a schedule of the salar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242434</th>\n",
       "      <td>This is a trade with OIL-SPEC-HEDGE-NG (John L...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242435</th>\n",
       "      <td>Some of my position is with the Alberta Term b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242436</th>\n",
       "      <td>2 ---------- From: Doucet, Dawn Sent: Wednesda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242437</th>\n",
       "      <td>Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242438</th>\n",
       "      <td>i think the YMCA has a class that is for peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242439 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0                                    Here is our forecast      0\n",
       "1       Traveling to have a business meeting takes the...      0\n",
       "2                           test successful. way to go!!!      0\n",
       "3       Randy, Can you send me a schedule of the salar...      0\n",
       "4                       Let's shoot for Tuesday at 11:45.      0\n",
       "...                                                   ...    ...\n",
       "242434  This is a trade with OIL-SPEC-HEDGE-NG (John L...      0\n",
       "242435  Some of my position is with the Alberta Term b...      0\n",
       "242436  2 ---------- From: Doucet, Dawn Sent: Wednesda...      0\n",
       "242437  Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...      0\n",
       "242438  i think the YMCA has a class that is for peopl...      0\n",
       "\n",
       "[242439 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f32f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/bayes2003/emails-for-spam-or-ham-classification-trec-2007?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483M/483M [00:36<00:00, 13.8MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\bayes2003\\emails-for-spam-or-ham-classification-trec-2007\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bayes2003/emails-for-spam-or-ham-classification-trec-2007\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a9beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_trec = r\"C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\bayes2003\\emails-for-spam-or-ham-classification-trec-2007\\versions\\1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba96232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34da0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_dataset_trec = os.listdir(dataset_path_trec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9296bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email_origin.csv\n",
      "**********\n",
      "email_text.csv\n",
      "**********\n",
      "trec07p.tgz\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for file in files_in_dataset_trec:\n",
    "      print(file)\n",
    "      print('*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb94bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trec = pd.read_csv(os.path.join(dataset_path_trec,\"email_text.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "829ab069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>do you feel the pressure to perform and not ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hi i've just updated from the gulus and i chec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mega authenticv i a g r a discount pricec i a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hey billy it was really fun going out the othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>system of the home it will have the capabiliti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53663</th>\n",
       "      <td>1</td>\n",
       "      <td>versuchen sie unser produkt und sie werden fuh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53664</th>\n",
       "      <td>1</td>\n",
       "      <td>while we may have high expectations of our ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53665</th>\n",
       "      <td>0</td>\n",
       "      <td>for those who are interested i just cook a lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53666</th>\n",
       "      <td>0</td>\n",
       "      <td>hello as i wrote i call sqlfetch channel t stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53667</th>\n",
       "      <td>1</td>\n",
       "      <td>well are implicated when does not the stage wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53668 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          1  do you feel the pressure to perform and not ri...\n",
       "1          0  hi i've just updated from the gulus and i chec...\n",
       "2          1  mega authenticv i a g r a discount pricec i a ...\n",
       "3          1  hey billy it was really fun going out the othe...\n",
       "4          1  system of the home it will have the capabiliti...\n",
       "...      ...                                                ...\n",
       "53663      1  versuchen sie unser produkt und sie werden fuh...\n",
       "53664      1  while we may have high expectations of our ass...\n",
       "53665      0  for those who are interested i just cook a lit...\n",
       "53666      0  hello as i wrote i call sqlfetch channel t stu...\n",
       "53667      1  well are implicated when does not the stage wh...\n",
       "\n",
       "[53668 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "829eb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trec = df_trec.rename(columns={'text': 'message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eea2a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df, df_trec], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c84e70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31ce13e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suz's temporary replacement can't find the Mat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>continental.com Specials for john arnold Tuesd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>----------------------  Gerald Nemec/HOU/ECT o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no hidden fees and additional charges http lan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can you email the most recent version of the Q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296102</th>\n",
       "      <td>Louise /Dave: Fyi, see Theresa's email below. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296103</th>\n",
       "      <td>it is true that escapenumber of women want it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296104</th>\n",
       "      <td>Start Date: 4/6/01; HourAhead hour: 16; No anc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296105</th>\n",
       "      <td>In light of Kay's comments of last evening and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296106</th>\n",
       "      <td>Phillip Allen will put 50 Million in Schedule ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296107 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0       Suz's temporary replacement can't find the Mat...      0\n",
       "1       continental.com Specials for john arnold Tuesd...      0\n",
       "2       ----------------------  Gerald Nemec/HOU/ECT o...      0\n",
       "3       no hidden fees and additional charges http lan...      1\n",
       "4       can you email the most recent version of the Q...      0\n",
       "...                                                   ...    ...\n",
       "296102  Louise /Dave: Fyi, see Theresa's email below. ...      0\n",
       "296103   it is true that escapenumber of women want it...      1\n",
       "296104  Start Date: 4/6/01; HourAhead hour: 16; No anc...      0\n",
       "296105  In light of Kay's comments of last evening and...      0\n",
       "296106  Phillip Allen will put 50 Million in Schedule ...      0\n",
       "\n",
       "[296107 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9721ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    266184\n",
      "1     29923\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_combined['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "252355cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emails = pd.read_csv(r\"C:\\Users\\LENOVO\\Downloads\\merged_emails (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "626779cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Young Esposito &lt;Young@iworld.de&gt;</td>\n",
       "      <td>user4@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 16:31:02 -0700</td>\n",
       "      <td>Never agree to be a loser</td>\n",
       "      <td>Buck up, your troubles caused by small dimensi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mok &lt;ipline's1983@icable.ph&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 18:31:03 -0500</td>\n",
       "      <td>Befriend Jenna Jameson</td>\n",
       "      <td>\\nUpgrade your sex and pleasures with these te...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Top 10 &lt;Karmandeep-opengevl@universalnet...</td>\n",
       "      <td>user2.9@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 20:28:00 -1200</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Parker &lt;ivqrnai@pobox.com&gt;</td>\n",
       "      <td>SpamAssassin Dev &lt;xrh@spamassassin.apache.org&gt;</td>\n",
       "      <td>Tue, 05 Aug 2008 17:31:20 -0600</td>\n",
       "      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n",
       "      <td>Would anyone object to removing .so from this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gretchen Suggs &lt;externalsep1@loanofficertool.com&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 19:31:21 -0400</td>\n",
       "      <td>SpecialPricesPharmMoreinfo</td>\n",
       "      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137672</th>\n",
       "      <td>SCC &lt;Gerry.Rossi4360@kinki-kids.com&gt;</td>\n",
       "      <td>Deficient &lt;deficient@flax9.uwaterloo.ca&gt;</td>\n",
       "      <td>Fri, 06 Jul 2007 06:53:36 -0400</td>\n",
       "      <td>Job: just for you.</td>\n",
       "      <td>\\n\\n\\n\\nWhile      we  may       have    high ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137673</th>\n",
       "      <td>Sydney Car Centre &lt;Merrill8783@168city.com&gt;</td>\n",
       "      <td>Gnitpick &lt;gnitpick@flax9.uwaterloo.ca&gt;</td>\n",
       "      <td>Fri, 06 Jul 2007 06:59:51 -0400</td>\n",
       "      <td>the reply for your request for a job place [le...</td>\n",
       "      <td>\\n\\n\\n\\nWhile     we       may    have high   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137674</th>\n",
       "      <td>Philippe Grosjean &lt;phgrosjean@sciviews.org&gt;</td>\n",
       "      <td>Duncan Murdoch &lt;murdoch@stats.uwo.ca&gt;</td>\n",
       "      <td>Fri, 06 Jul 2007 12:57:17 +0200</td>\n",
       "      <td>Re: [R] Me again, about the horrible documenta...</td>\n",
       "      <td>For those who are interested, I just cook a li...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137675</th>\n",
       "      <td>Bernhard Wellhöfer &lt;Bernhard.Wellhoefer@gaia-g...</td>\n",
       "      <td>r-help@stat.math.ethz.ch</td>\n",
       "      <td>Fri, 06 Jul 2007 12:43:12 +0200</td>\n",
       "      <td>Re: [R] RODBC problem</td>\n",
       "      <td>Hello,\\n\\nas I wrote I call\\n\\n  sqlFetch(chan...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137676</th>\n",
       "      <td>Danny &lt;pwcusnt@noblecoffee.com&gt;</td>\n",
       "      <td>the00@plg.uwaterloo.ca</td>\n",
       "      <td>Fri, 06 Jul 2007 18:04:14 +0700</td>\n",
       "      <td>I wanted the desk at his own laws: of the.  Bu...</td>\n",
       "      <td>Well, are implicated.  When does not the stage...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137677 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sender  \\\n",
       "0                        Young Esposito <Young@iworld.de>   \n",
       "1                            Mok <ipline's1983@icable.ph>   \n",
       "2       Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
       "3                      Michael Parker <ivqrnai@pobox.com>   \n",
       "4       Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
       "...                                                   ...   \n",
       "137672               SCC <Gerry.Rossi4360@kinki-kids.com>   \n",
       "137673        Sydney Car Centre <Merrill8783@168city.com>   \n",
       "137674        Philippe Grosjean <phgrosjean@sciviews.org>   \n",
       "137675  Bernhard Wellhöfer <Bernhard.Wellhoefer@gaia-g...   \n",
       "137676                    Danny <pwcusnt@noblecoffee.com>   \n",
       "\n",
       "                                              receiver  \\\n",
       "0                          user4@gvc.ceas-challenge.cc   \n",
       "1                        user2.2@gvc.ceas-challenge.cc   \n",
       "2                        user2.9@gvc.ceas-challenge.cc   \n",
       "3       SpamAssassin Dev <xrh@spamassassin.apache.org>   \n",
       "4                        user2.2@gvc.ceas-challenge.cc   \n",
       "...                                                ...   \n",
       "137672        Deficient <deficient@flax9.uwaterloo.ca>   \n",
       "137673          Gnitpick <gnitpick@flax9.uwaterloo.ca>   \n",
       "137674           Duncan Murdoch <murdoch@stats.uwo.ca>   \n",
       "137675                        r-help@stat.math.ethz.ch   \n",
       "137676                          the00@plg.uwaterloo.ca   \n",
       "\n",
       "                                   date  \\\n",
       "0       Tue, 05 Aug 2008 16:31:02 -0700   \n",
       "1       Tue, 05 Aug 2008 18:31:03 -0500   \n",
       "2       Tue, 05 Aug 2008 20:28:00 -1200   \n",
       "3       Tue, 05 Aug 2008 17:31:20 -0600   \n",
       "4       Tue, 05 Aug 2008 19:31:21 -0400   \n",
       "...                                 ...   \n",
       "137672  Fri, 06 Jul 2007 06:53:36 -0400   \n",
       "137673  Fri, 06 Jul 2007 06:59:51 -0400   \n",
       "137674  Fri, 06 Jul 2007 12:57:17 +0200   \n",
       "137675  Fri, 06 Jul 2007 12:43:12 +0200   \n",
       "137676  Fri, 06 Jul 2007 18:04:14 +0700   \n",
       "\n",
       "                                                  subject  \\\n",
       "0                               Never agree to be a loser   \n",
       "1                                  Befriend Jenna Jameson   \n",
       "2                                    CNN.com Daily Top 10   \n",
       "3       Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       "4                              SpecialPricesPharmMoreinfo   \n",
       "...                                                   ...   \n",
       "137672                                 Job: just for you.   \n",
       "137673  the reply for your request for a job place [le...   \n",
       "137674  Re: [R] Me again, about the horrible documenta...   \n",
       "137675                              Re: [R] RODBC problem   \n",
       "137676  I wanted the desk at his own laws: of the.  Bu...   \n",
       "\n",
       "                                                     body  label  urls  \n",
       "0       Buck up, your troubles caused by small dimensi...      1   1.0  \n",
       "1       \\nUpgrade your sex and pleasures with these te...      1   1.0  \n",
       "2       >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1   1.0  \n",
       "3       Would anyone object to removing .so from this ...      0   1.0  \n",
       "4       \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1   1.0  \n",
       "...                                                   ...    ...   ...  \n",
       "137672  \\n\\n\\n\\nWhile      we  may       have    high ...      1   1.0  \n",
       "137673  \\n\\n\\n\\nWhile     we       may    have high   ...      1   1.0  \n",
       "137674  For those who are interested, I just cook a li...      0   1.0  \n",
       "137675  Hello,\\n\\nas I wrote I call\\n\\n  sqlFetch(chan...      0   1.0  \n",
       "137676  Well, are implicated.  When does not the stage...      1   0.0  \n",
       "\n",
       "[137677 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bca0fb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls'], dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6245ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emails_re = merged_emails.drop(columns = ['sender', 'receiver', 'date', 'subject','urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eec0b74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buck up, your troubles caused by small dimensi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nUpgrade your sex and pleasures with these te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Would anyone object to removing .so from this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137672</th>\n",
       "      <td>\\n\\n\\n\\nWhile      we  may       have    high ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137673</th>\n",
       "      <td>\\n\\n\\n\\nWhile     we       may    have high   ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137674</th>\n",
       "      <td>For those who are interested, I just cook a li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137675</th>\n",
       "      <td>Hello,\\n\\nas I wrote I call\\n\\n  sqlFetch(chan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137676</th>\n",
       "      <td>Well, are implicated.  When does not the stage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137677 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body  label\n",
       "0       Buck up, your troubles caused by small dimensi...      1\n",
       "1       \\nUpgrade your sex and pleasures with these te...      1\n",
       "2       >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1\n",
       "3       Would anyone object to removing .so from this ...      0\n",
       "4       \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1\n",
       "...                                                   ...    ...\n",
       "137672  \\n\\n\\n\\nWhile      we  may       have    high ...      1\n",
       "137673  \\n\\n\\n\\nWhile     we       may    have high   ...      1\n",
       "137674  For those who are interested, I just cook a li...      0\n",
       "137675  Hello,\\n\\nas I wrote I call\\n\\n  sqlFetch(chan...      0\n",
       "137676  Well, are implicated.  When does not the stage...      1\n",
       "\n",
       "[137677 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_emails_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbfa24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emails_re = merged_emails_re.rename(columns={'body': 'message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c032b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_combined, merged_emails_re], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d7095af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc34237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nAt this moment, an ingenious and animating...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The version of R on our unix system has been u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i think you should go soon ---------- From: Ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i haven't really thought of that yet. i guess ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433779</th>\n",
       "      <td>Threat levels: DOE is still at Security Condit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433780</th>\n",
       "      <td>do n't want to receive these e-mails ? click t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433781</th>\n",
       "      <td>The following reports have been waiting for yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433782</th>\n",
       "      <td>FYI ---------- enron.com Subject: Paid Survey ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433783</th>\n",
       "      <td>=09 [IMAGE][IMAGE] [IMAGE] =09 [IMAGE] Home [I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433784 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0       \\n\\nAt this moment, an ingenious and animating...      1\n",
       "1       >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1\n",
       "2       The version of R on our unix system has been u...      0\n",
       "3       i think you should go soon ---------- From: Ne...      0\n",
       "4       i haven't really thought of that yet. i guess ...      0\n",
       "...                                                   ...    ...\n",
       "433779  Threat levels: DOE is still at Security Condit...      0\n",
       "433780  do n't want to receive these e-mails ? click t...      1\n",
       "433781  The following reports have been waiting for yo...      0\n",
       "433782  FYI ---------- enron.com Subject: Paid Survey ...      0\n",
       "433783  =09 [IMAGE][IMAGE] [IMAGE] =09 [IMAGE] Home [I...      0\n",
       "\n",
       "[433784 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5bbbc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    333136\n",
       "1    100648\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7d65be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1812)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "821d4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicates: (431972, 2)\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.drop_duplicates().reset_index(drop=True)\n",
    "print(\"After removing duplicates:\", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0b9d838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    331327\n",
       "1    100645\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d9f63",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28d2606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nlpaug) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nlpaug) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nlpaug) (2.32.3)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown, nlpaug\n",
      "\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   -------------------- ------------------- 1/2 [nlpaug]\n",
      "   ---------------------------------------- 2/2 [nlpaug]\n",
      "\n",
      "Successfully installed gdown-5.2.0 nlpaug-1.1.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nlpaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e816d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    331327\n",
      "1    130838\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import pandas as pd\n",
    "\n",
    "# Create synonym replacement augmenter\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Take your spam subset\n",
    "spam_df = df_new[df_new['label'] == 1]\n",
    "\n",
    "# Augment a portion (say 30%)\n",
    "aug_samples = int(len(spam_df) * 0.3)\n",
    "spam_texts = spam_df['message'].sample(aug_samples, random_state=42)\n",
    "\n",
    "# Generate augmented text\n",
    "augmented_texts = [aug.augment(t) for t in spam_texts]\n",
    "\n",
    "# Combine into DataFrame\n",
    "aug_df = pd.DataFrame({'message': augmented_texts, 'label': 1})\n",
    "\n",
    "# Merge back with the original data\n",
    "df_augmented = pd.concat([df_new, aug_df], ignore_index=True)\n",
    "print(df_augmented['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "565784a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eefe661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['-----BEGIN PGP SIGNED MESSAGE-----\\nHash: SHA1\\n\\n\\n\\nThe Thursday 2008-02-21 at 03:51 -0800, Tommy Pham wrote:\\n\\n(please, trim the quotes)\\n\\n>> Things are not so simple, because these modern 32 bit processors have \\n>> in fact 36 address lines, being capable of addressing 64 GiB, not 4. \\n>> With tricks, of course, like PAE (*), because the registers are 32 bit \\n>> wide.\\n>>\\n>> * http://en.wikipedia.org/wiki/Physical_Address_Extension\\n>>\\n\\n> True, you can use (PAE) extensions but inherently being 64 bit is\\n> faster since you have more \"lanes\" to address those memory right?\\n\\nI haven\\'t said anything to the contrary. Simply that modern 32 bit \\npentiums can adress 36 address lines, that\\'s the hardware. Similarly, the \\noriginal x86 had 20 address lines, although the address registers had only \\n16 bits - due to the use of segment registers.\\n\\n- -- \\nCheers,\\n        Carlos E. R.\\n\\n-----BEGIN PGP SIGNATURE-----\\nVersion: GnuPG v2.0.4-svn0 (GNU/Linux)\\n\\niD8DBQFHvXXrtTMYHG2NR9URAkbCAKCOSFb4ac4pqLx8aDz/19o5pdzrkQCeKvY/\\npB0P09jgfPWMKWN56J8eioY=\\n=WdzZ\\n-----END PGP SIGNATURE-----\\n\\n\\n'],\n",
       "       ['This is a multi-part message in MIME format. ------=_NextPart_000_181FC9_01C263E7.DE66F600 Content-Type: text/plain; \\tcharset=\"Windows-1252\" Content-Transfer-Encoding: quoted-printable =20  Financial Power You Can Depend On =20  Very Competitive Rates=0A= Guaranteed 6 Years  Let AIG\\'s Annuity Portfolio Work for You!=09 A.M. Best Company\\t \"A+\" Superior\\t=20 Standard & Poor\\'s Corp.\\t \"AA+\" Very Strong\\t=20 Fitch\\t \"AA+\" Very Strong\\t=20 Moody\\'s Investors Service\\t \"Aa1\" Excellent\\t=20 Call today for more information!  888-237-4210 - or - Please fill out the form below for more information\\t=20 Name:\\t  \\t =09 E-mail:\\t  =09 Phone:\\t  =09 City:\\t  \\tState:\\t  =09  \\t    =09 =20  AIG Annuity Insurance Company We don\\'t want anyone to receive our mailings who does not wish to receive them. This is a professional communication sent to insurance professionals. To be removed from this mailing list, DO NOT REPLY to this message. Instead, go here: insuranceiq.com/optout =20 Legal Notice =20 ------=_NextPart_000_181FC9_01C263E7.DE66F600 Content-Type: text/html; \\tcharset=\"iso-8859-1\" Content-Transfer-Encoding: quoted-printable Financial Power You Can Depend On =20 =20      =20            =20                  =20                 A.M. Best = Company \"A+\" = Superior =20                 Standard & Poor\\'s = Corp. \"AA+\" Very = Strong =20                 Fitch \"AA+\" Very = Strong =20                 Moody\\'s Investors = Service \"Aa1\" = Excellent Call today for more = information!             - or - =20                  =20                      =20                         Please fill out the form below for more = information =20                         Name: =20                            =20                         E-mail: =20                            =20                         Phone: =20                            =20                         City: =20                            State: =20                            =20                         \\xa0 =20                            =20      We don\\'t = want anyone to receive our mailings who does not=20 \\t\\t\\twish to receive them. This is a professional communication=20 \\t\\t\\tsent to insurance professionals. To be removed from this mailing=20 \\t\\t\\tlist, DO NOT REPLY to this message. Instead, go here: =20 \\t\\t\\tinsuranceiq.com/optout Legal Notice =20 ------=_NextPart_000_181FC9_01C263E7.DE66F600--'],\n",
       "       ['it is going to be huge special situation alert tmxo trimax are providers of broadband over power line bpl communication technologies otc tmxo last escapenumber escapenumber technologies that use the power grid to deliver escapenumber bit encrypted high speed symmetrical broadband for data voice and video transmission this is a sector to be in all material herein were prepared by us based upon information believed to be reliable but not guaranteed to be accurate and should not be considered to be all inclusive this opinion contains forward looking statements that involve risks and uncertainties you could lose all your money we are not a licensed broker broker dealer market maker investment banker investment advisor analyst or underwriter please consult a broker before purchasing or selling any securities viewed or mentioned herein we are negotiating a cash price for this advertisement in the near future but at this time have received nothing third parties affiliates officers directors and employees may also own or may buy the shares discussed in this opinion and intend to sell or profit in the event those shares rise or decrease in value is escapenumber profit potential short term see bullish news online right now theescapenumber call broker'],\n",
       "       ['If you are looking to feed and challenge your faith, visit Crosswalk.com\\'s Spiritual Life Channel at: http://spiritual.crosswalk.com ----------- N E I L A N D E R S O N D A I L Y D E V O T I O N A L from Freedom in Christ Ministries October 19 DON\\'T BELIEVE IT Do not turn to mediums or spiritists; do not seek them out to be defiled by them (Leviticus 19:31). Where do mediums and spiritists get their \"amazing\" information and insights? Many of them are demonic channelers, but much of what is called \"spiritism\" and \"psychic phenomena\" is no more than clever illusion. hese so-called spiritists give what is referred to as \"cold readings.\" You go to them for advice or direction, and they ask you a few simple, leading questions. Based on the information you give, they make general observations which are probably true of most people in your situation. But you\\'re so impressed with the accuracy of their \"revelations\" that you start tipping them off to all kinds of details which they can fabricate into their \"reading.\" This is not demonic; it\\'s just mental and verbal sleight-of-hand. But the mediums and spiritists that God warned against in Leviticus and Deuteronomy were not con artists, but people who possessed and passed on knowledge which didn\\'t come through natural channels of perception. These people have opened themselves up to the spirit world and have become channels of knowledge from Satan. The charlatan with his phony cold readings is only interested in bilking you of your money. But the false knowledge and direction which comes from Satan through a medium is intended to bilk you of your spiritual vitality and freedom. There\\'s big money in psychic/con artist operations, and a lot of magicians are raking it in. Many people crave to know something extra about their lives and their future, and they will pay handsomely if they think you can give them the inside information they desire. When a psychic claims to have contacted the dead, don\\'t believe it. When a psychologist claims to have regressed a client back to a former existence through hypnosis, don\\'t believe it. When a New Age medium purports to channel a person from the past into the present, realize that it is nothing more than a familiar spirit or the fraudulent work of a con artist. Lord Jesus, You are the way, the truth and the life. I renounce any power or revelation from any other source than You. ----------- This daily devotional is published and distributed by Crosswalk.com. It is written by Neil Anderson at < http://www.ficm.org >. You can purchase \"Daily in Christ\" and other titles by Neil Anderson at christianbook.com (Christian Book Distributors - CBD). < http://shopping.crosswalk.com/bye/devo_na > Additional devotionals are available from Crosswalk.com: < http://devotionals.crosswalk.com >. ----------- To read the Current Feature Story from ReligionToday, go to http://religiontoday.crosswalk.com/CurrentFeatureStory/ For today\\'s complete News Summary from ReligionToday, go to http://religiontoday.crosswalk.com/CurrentNewsSummary/ ----------- For Copyright Terms go to http://www.crosswalk.com/info/copyright ____________________SUBSCRIPTION INFO_______________________ * You subscribed to Neil Anderson as: < jeff.king@enron.com > * To unsubscribe from this newsletter immediately, click here: < http://link.crosswalk.com/UM/U.ASP?A3.14.261895 > If that link is not clickable, simply copy and paste it into your browser window. * To change your address: unsubscribe your old address and then subscribe your new address. * To subscribe, send an email to: < SUBSCRIBE-neilanderson@lists.crosswalk.com > or go to < http://www.crosswalk.com/lists > for a complete list of our newsletters. * Copyright ? 2001 Crosswalk.com, Inc. and its Content Providers. All rights reserved. ____________________________________________________________'],\n",
       "       [\">+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+= >THE DAILY TOP 10 >from CNN.com >Top videos and stories as of: Aug  1, 2008  3:58 PM EDT >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+= TOP 10 VIDEOS 1. PARIS HILTON TAKES ON MCCAIN http://www.cnn.com/video/partners/email/index.html?url=/video/politics/2008/08/06/wynter.paris.hilton.ad.cnn Paris Hilton swings back at Republican presidential candidate John McCain. Kareen Wynter reports. 2. BIKINI BARISTA STAND CLOSED http://www.cnn.com/video/partners/email/index.html?url=/video/living/2008/08/06/pkg.bikini.baristas.barred.kiro 3. TOT GRANDMA REACTS TO CHARGES http://www.cnn.com/video/partners/email/index.html?url=/video/crime/2008/08/06/grace.mom.charged.cnn 4. MAGGOTS: THE NEW ANTIBIOTIC? http://www.cnn.com/video/partners/email/index.html?url=/video/health/2008/08/06/mcginty.uk.maggot.antibiotic.itn 5. ON YOUR MARK, GET SET, FLOAT! http://www.cnn.com/video/partners/email/index.html?url=/video/living/2008/08/06/vo.in.balloon.race.wxin 6. 'LOVE TRIANGLE MURDER' http://www.cnn.com/video/partners/email/index.html?url=/video/crime/2008/08/06/dcl.london.in.session.trial.update.insession 7. TERI GARR'S DECADES OF HIDING http://www.cnn.com/video/partners/email/index.html?url=/video/health/2008/08/06/gupta.teri.garr.cnn 8. FAKE SEX AD DRAWS LAWSUIT http://www.cnn.com/video/partners/email/index.html?url=/video/tech/2008/08/06/dnt.wa.sex.ad.lawsuit.komo 9. 'WE NEED AN ECONOMIC SURGE' http://www.cnn.com/video/partners/email/index.html?url=/video/politics/2008/08/06/sot.mccain.econo.surge.cnn 10. OBAMA GOT HIT IN THE HEAD? http://www.cnn.com/video/partners/email/index.html?url=/video/politics/2008/08/06/sot.obama.asked.why.running.cnn TOP 10 STORIES 1. N. DAKOTA'S REAL-LIFE JED CLAMPETT http://www.cnn.com/2008/LIVING/wayoflife/08/05/oil.boomtown/index.html In the midst of a N. Dakota oil boom, a man born during the Great Depression is making a fortune after striking oil on his property. 2. OBAMA'S UPHILL POLLING BATTLE http://www.cnn.com/2008/POLITICS/08/06/obama.polls/index.html 3. D.A.: SEX CASE ABOUT 'PURE EVIL' http://www.cnn.com/2008/CRIME/08/06/sex.club.trial.ap/index.html 4. ANOTHER SIDE OF AMY RAY http://www.cnn.com/2008/SHOWBIZ/Music/08/06/amy.ray/index.html 5. COMMENTARY: BILL CLINTON'S UPSET http://www.cnn.com/2008/US/08/06/martin.billclinton/index.html 6. WOMEN DRIVEN TO DONATE EGGS http://www.cnn.com/2008/HEALTH/08/05/selling.eggs/index.html 7. ANTHRAX SUSPECT CALLED MISLEADING http://www.cnn.com/2008/CRIME/08/06/anthrax.case/index.html 8. MOUNTAIN LION IN BEDROOM KILLS DOG http://www.cnn.com/2008/US/08/06/mountain.lion.ap/index.html 9. REPORTS: MISTAKES DOOMED UTAH MINE http://www.cnn.com/2008/US/08/06/utah.mine.anniv.ap/index.html 10. BIN LADEN'S FORMER DRIVER GUILTY http://www.cnn.com/2008/CRIME/08/06/hamdan.trial/index.html CNN, The Most Trusted Name in News >    Cable News Network LP, LLLP.                       < >    One CNN Center, Atlanta, Georgia 30303             < >    2008 Cable News Network LP, LLLP.                  < >    A Time Warner Company.                             < >    All Rights Reserved.                               < ========================================================= =   Please send comments or suggestions by going to     = =           http://www.cnn.com/feedback/                = =                                                       = =      Read our privacy guidelines by going to          = =         http://www.cnn.com/privacy.html               = ========================================================= You have agreed to receive this email from CNN.com as a result of your CNN.com preference settings. To manage your settings, go to: http://www.cnn.com/linkto/bn.manage.html To unsubscribe from the Daily Top 10, go to http://cgi.cnn.com/m/clik?e=email1867@gvc.ceas-challenge.cc&l=cnn-dailytop10\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented[['message']].sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08e558df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.7.14)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed17c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "def back_translate(text, lang='fr'):\n",
    "    try:\n",
    "        # Step 1: English → French (or another language)\n",
    "        translated = GoogleTranslator(source='auto', target=lang).translate(text)\n",
    "        # Step 2: French → English\n",
    "        back_translated = GoogleTranslator(source=lang, target='en').translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        return text  # fallback to original if API fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5e6bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m augmented_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(spam_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 11\u001b[0m     new_msg \u001b[38;5;241m=\u001b[39m back_translate(msg)\n\u001b[0;32m     12\u001b[0m     augmented_texts\u001b[38;5;241m.\u001b[39mappend(new_msg)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Print progress every 100 samples\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[71], line 11\u001b[0m, in \u001b[0;36mback_translate\u001b[1;34m(text, lang)\u001b[0m\n\u001b[0;32m      9\u001b[0m     translated \u001b[38;5;241m=\u001b[39m GoogleTranslator(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39mlang)\u001b[38;5;241m.\u001b[39mtranslate(text)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Step 2: French → English\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     back_translated \u001b[38;5;241m=\u001b[39m GoogleTranslator(source\u001b[38;5;241m=\u001b[39mlang, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate(translated)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m back_translated\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\deep_translator\\google.py:67\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayload_key:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayload_key] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_params, proxies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1063\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1219\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1216\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1138\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select spam samples\n",
    "spam_df_new = df_new[df_new['label'] == 1]\n",
    "\n",
    "# Randomly choose a few thousand\n",
    "sample_size = 5000\n",
    "spam_sample = spam_df_new.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Apply back translation\n",
    "augmented_texts = []\n",
    "for i, msg in enumerate(spam_sample['message']):\n",
    "    new_msg = back_translate(msg)\n",
    "    augmented_texts.append(new_msg)\n",
    "    \n",
    "    # Print progress every 100 samples\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i}/{sample_size}\")\n",
    "    time.sleep(0.5)  # add delay to avoid rate limits\n",
    "\n",
    "# Build augmented dataframe\n",
    "backtrans_df = pd.DataFrame({'message': augmented_texts, 'label': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_augmented, backtrans_df], ignore_index=True)\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df_final['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75f7ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/jackksoncsie/spam-email-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.86M/2.86M [00:02<00:00, 1.42MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\jackksoncsie\\spam-email-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jackksoncsie/spam-email-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a92b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_3 = r\"C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\jackksoncsie\\spam-email-dataset\\versions\\1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e6a6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_dataset_3 = os.listdir(dataset_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ee938ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails.csv\n"
     ]
    }
   ],
   "source": [
    "for files in files_in_dataset_3:\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a00ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(os.path.join(dataset_path_3,\"emails.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b757330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>Subject: re : research and development charges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5724</th>\n",
       "      <td>Subject: re : receipts from visit  jim ,  than...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>Subject: re : enron case study update  wow ! a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>Subject: re : interest  david ,  please , call...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>Subject: news : aurora 5 . 2 update  aurora ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  spam\n",
       "0     Subject: naturally irresistible your corporate...     1\n",
       "1     Subject: the stock trading gunslinger  fanny i...     1\n",
       "2     Subject: unbelievable new homes made easy  im ...     1\n",
       "3     Subject: 4 color printing special  request add...     1\n",
       "4     Subject: do not have money , get software cds ...     1\n",
       "...                                                 ...   ...\n",
       "5723  Subject: re : research and development charges...     0\n",
       "5724  Subject: re : receipts from visit  jim ,  than...     0\n",
       "5725  Subject: re : enron case study update  wow ! a...     0\n",
       "5726  Subject: re : interest  david ,  please , call...     0\n",
       "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0\n",
       "\n",
       "[5728 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f69fb64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam\n",
       "0    4360\n",
       "1    1368\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b221c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_3.rename(columns={'spam': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "daa6de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_3.rename(columns={'text': 'message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0249f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    331327\n",
      "1    132206\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter spam emails from df_3\n",
    "df_3_spam = df_3[df_3['label'] == 1]\n",
    "\n",
    "# Merge with your augmented dataset\n",
    "df_final = pd.concat([df_augmented, df_3_spam], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows for randomness\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify final distribution\n",
    "print(df_final['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d82eb7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/meruvulikith/190k-spam-ham-email-dataset-for-classification?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107M/107M [00:09<00:00, 11.2MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\meruvulikith\\190k-spam-ham-email-dataset-for-classification\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"meruvulikith/190k-spam-ham-email-dataset-for-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2e76f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_190k = r\"C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\meruvulikith\\190k-spam-ham-email-dataset-for-classification\\versions\\1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b83af7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(dataset_path_190k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e9a8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_Emails_data.csv\n"
     ]
    }
   ],
   "source": [
    "for files in files :\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04500dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_190 = pd.read_csv(os.path.join(dataset_path_190k,\"spam_Emails_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db95582b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spam</td>\n",
       "      <td>viiiiiiagraaaa\\nonly for the ones that want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ham</td>\n",
       "      <td>got ice thought look az original message ice o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spam</td>\n",
       "      <td>yo ur wom an ne eds an escapenumber in ch ma n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spam</td>\n",
       "      <td>start increasing your odds of success &amp; live s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ham</td>\n",
       "      <td>author jra date escapenumber escapenumber esca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193847</th>\n",
       "      <td>Ham</td>\n",
       "      <td>on escapenumber escapenumber escapenumber rob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193848</th>\n",
       "      <td>Spam</td>\n",
       "      <td>we have everything you need escapelong cialesc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193849</th>\n",
       "      <td>Ham</td>\n",
       "      <td>hi quick question say i have a date variable i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193850</th>\n",
       "      <td>Spam</td>\n",
       "      <td>thank you for your loan request which we recie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193851</th>\n",
       "      <td>Ham</td>\n",
       "      <td>this is an automatically generated delivery st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193852 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0       Spam  viiiiiiagraaaa\\nonly for the ones that want to...\n",
       "1        Ham  got ice thought look az original message ice o...\n",
       "2       Spam  yo ur wom an ne eds an escapenumber in ch ma n...\n",
       "3       Spam  start increasing your odds of success & live s...\n",
       "4        Ham  author jra date escapenumber escapenumber esca...\n",
       "...      ...                                                ...\n",
       "193847   Ham  on escapenumber escapenumber escapenumber rob ...\n",
       "193848  Spam  we have everything you need escapelong cialesc...\n",
       "193849   Ham  hi quick question say i have a date variable i...\n",
       "193850  Spam  thank you for your loan request which we recie...\n",
       "193851   Ham  this is an automatically generated delivery st...\n",
       "\n",
       "[193852 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d6ac0516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    102160\n",
      "1     91692\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map textual labels to numeric\n",
    "df_190['label'] = df_190['label'].map({'Spam': 1, 'Ham': 0})\n",
    "\n",
    "# Verify the result\n",
    "print(df_190['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "038c5150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>viiiiiiagraaaa\\nonly for the ones that want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>got ice thought look az original message ice o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>yo ur wom an ne eds an escapenumber in ch ma n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>start increasing your odds of success &amp; live s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>author jra date escapenumber escapenumber esca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193847</th>\n",
       "      <td>0</td>\n",
       "      <td>on escapenumber escapenumber escapenumber rob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193848</th>\n",
       "      <td>1</td>\n",
       "      <td>we have everything you need escapelong cialesc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193849</th>\n",
       "      <td>0</td>\n",
       "      <td>hi quick question say i have a date variable i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193850</th>\n",
       "      <td>1</td>\n",
       "      <td>thank you for your loan request which we recie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193851</th>\n",
       "      <td>0</td>\n",
       "      <td>this is an automatically generated delivery st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193852 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0           1  viiiiiiagraaaa\\nonly for the ones that want to...\n",
       "1           0  got ice thought look az original message ice o...\n",
       "2           1  yo ur wom an ne eds an escapenumber in ch ma n...\n",
       "3           1  start increasing your odds of success & live s...\n",
       "4           0  author jra date escapenumber escapenumber esca...\n",
       "...       ...                                                ...\n",
       "193847      0  on escapenumber escapenumber escapenumber rob ...\n",
       "193848      1  we have everything you need escapelong cialesc...\n",
       "193849      0  hi quick question say i have a date variable i...\n",
       "193850      1  thank you for your loan request which we recie...\n",
       "193851      0  this is an automatically generated delivery st...\n",
       "\n",
       "[193852 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "26d4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_190 = df_190.rename(columns={'text': 'message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ac7eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_190['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d6c2520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    331327\n",
      "1    223898\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter spam emails from df_3\n",
    "df_190_spam = df_190[df_190['label'] == 1]\n",
    "\n",
    "# Merge with your augmented dataset\n",
    "df_final_new = pd.concat([df_final, df_190_spam], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows for randomness\n",
    "df_final_new = df_final_new.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify final distribution\n",
    "print(df_final_new['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10cd9e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42761\n"
     ]
    }
   ],
   "source": [
    "print(df_final_new.duplicated(subset='message').sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "06df1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(42761)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_new.duplicated(subset = \"message\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "134dd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_new.to_csv(\"clean_spam_email_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "259ae10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_1 = df_final_new.drop_duplicates(subset='message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5f31fb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    331324\n",
       "1    181140\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "356d0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_to_keep = 280000\n",
    "df_ham = df_final_1[df_final_1['label'] == 0].sample(ham_to_keep, random_state=42)\n",
    "df_spam = df_final_1[df_final_1['label'] == 1]\n",
    "df_balanced = pd.concat([df_ham, df_spam]).sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "127cb94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are the provisions we care about in Title 9? M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fyi ---------- enron.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nFROM:MRS.GLORIA DUBE.\\n\\n  Good  Day\\n\\n    ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>continental com specials for martin cuilla tue...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on escapenumber escapenumber escapenumber siva...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461135</th>\n",
       "      <td>Elizabeth, Andrea and I are scheduled to meet ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461136</th>\n",
       "      <td>[erwin hepatica bundestag? tusk, psychotherapi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461137</th>\n",
       "      <td>Here's one for you to chew on for awhile. As w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461138</th>\n",
       "      <td>oneok partners l p event reminder oneok and on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461139</th>\n",
       "      <td>thanks to marc duncan tobias alberto i'm aware...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>461140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0       Are the provisions we care about in Title 9? M...      0\n",
       "1                                fyi ---------- enron.com      0\n",
       "2       \\nFROM:MRS.GLORIA DUBE.\\n\\n  Good  Day\\n\\n    ...      1\n",
       "3       continental com specials for martin cuilla tue...      1\n",
       "4       on escapenumber escapenumber escapenumber siva...      0\n",
       "...                                                   ...    ...\n",
       "461135  Elizabeth, Andrea and I are scheduled to meet ...      0\n",
       "461136  [erwin hepatica bundestag? tusk, psychotherapi...      1\n",
       "461137  Here's one for you to chew on for awhile. As w...      0\n",
       "461138  oneok partners l p event reminder oneok and on...      0\n",
       "461139  thanks to marc duncan tobias alberto i'm aware...      0\n",
       "\n",
       "[461140 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e949ba7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_balanced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_balanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_balanced' is not defined"
     ]
    }
   ],
   "source": [
    "df_balanced['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "949ef2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      " label\n",
      "0    0.607191\n",
      "1    0.392809\n",
      "Name: proportion, dtype: float64\n",
      "Val:\n",
      " label\n",
      "0    0.607191\n",
      "1    0.392809\n",
      "Name: proportion, dtype: float64\n",
      "Test:\n",
      " label\n",
      "0    0.607191\n",
      "1    0.392809\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Extract features and labels\n",
    "X = df_balanced['message']\n",
    "y = df_balanced['label']\n",
    "\n",
    "# Step 2: Split into train (70%), temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Split temp into validation (15%) and test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Combine back into dataframes if needed\n",
    "train_df = pd.DataFrame({'message': X_train, 'label': y_train})\n",
    "val_df   = pd.DataFrame({'message': X_val,   'label': y_val})\n",
    "test_df  = pd.DataFrame({'message': X_test,  'label': y_test})\n",
    "\n",
    "# Step 5: Check label balance in each split\n",
    "print(\"Train:\\n\", train_df['label'].value_counts(normalize=True))\n",
    "print(\"Val:\\n\", val_df['label'].value_counts(normalize=True))\n",
    "print(\"Test:\\n\", test_df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d9b555",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "10df4f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    196000\n",
       "1    126798\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1eac71d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69171, 2)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1cdd3c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running Step 1: preprocessing on train/val/test with verbose=1\n",
      "[preprocess] Starting preprocess on df with shape (322798, 2)\n",
      "[preprocess] Cleaned text (sample):\n",
      "[\"['THE BULL REPORT! !! Butt sym: CHVCCurrent: $ 0. 69 ane Day Target mary leontyne price: $ ace. 5Action: Potent Bargain / Storage area. This sym be gaining impulse. . CHVC has a nice fresh news, theorize, contact your broker!']\", 'Confidential I will send you updates Hunter S Shively/HOU/ECT on 08/04/2000 09:12 AM Enron North America Corp. ECT cc: Subject: Vector Pipeline write-up by D. Baughman.', 'HealthMedsCustomerSupport <URL>']\n",
      "[preprocess] Extracting features...\n",
      "[preprocess] Feature summary (first 5 rows):\n",
      "[{'_clean_text': \"['THE BULL REPORT! !! Butt sym: CHVCCurrent: $ 0. 69 ane Day Target mary leontyne price: $ ace. 5Action: Potent Bargain / Storage area. This sym be gaining impulse. . CHVC has a nice fresh news, theorize, contact your broker!']\", 'text_len': 227, 'num_words': 40, 'num_links': 0, 'num_uppercase_words': 4, 'num_exclamations': 4, 'num_spam_keywords': 0, 'avg_word_length': 4.9393939393939394, 'digit_ratio': 0.01762114537444934, 'has_currency_symbol': 1}, {'_clean_text': 'Confidential I will send you updates Hunter S Shively/HOU/ECT on 08/04/2000 09:12 AM Enron North America Corp. ECT cc: Subject: Vector Pipeline write-up by D. Baughman.', 'text_len': 168, 'num_words': 26, 'num_links': 0, 'num_uppercase_words': 3, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 4.444444444444445, 'digit_ratio': 0.07142857142857142, 'has_currency_symbol': 0}, {'_clean_text': 'HealthMedsCustomerSupport <URL>', 'text_len': 31, 'num_words': 2, 'num_links': 1, 'num_uppercase_words': 1, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 14.0, 'digit_ratio': 0.0, 'has_currency_symbol': 0}, {'_clean_text': 'On mahonia margining, I spoke with mike garberding. Tells me he went through all collateral issues on these deals with wendi lebrocq in credit last week and everything tied exactly. He will walk tanya rohauer through the exact same steps tomorrow. Eric boyt is pulling deal numbers and details to match to documents. Sent from my BlackBerry Wireless Handheld ( <URL>', 'text_len': 366, 'num_words': 61, 'num_links': 1, 'num_uppercase_words': 1, 'num_exclamations': 0, 'num_spam_keywords': 1, 'avg_word_length': 4.966666666666667, 'digit_ratio': 0.0, 'has_currency_symbol': 0}, {'_clean_text': \"begin pgp signed message hash shaescapenumber william jojo schrieb working with heikki and the aix builds we've discovered a runtime linking issue there is a reference to talloc zero in relative to source tree bin libsmbclient so bin libmsrpc so bin libaddns so this shounds correct bin rid so bin ad so rid so and ad so are modules in winbind and should get the talloc zero reference when loading at least they should as winind has talloc metze begin pgp signature version gnupg vescapenumber escapenumber escapenumber gnu linux comment using gnupg with suse http enigmail mozdev org escapelong escapelong escapelong unkescapenumber end pgp signature\", 'text_len': 651, 'num_words': 105, 'num_links': 0, 'num_uppercase_words': 0, 'num_exclamations': 0, 'num_spam_keywords': 1, 'avg_word_length': 5.150943396226415, 'digit_ratio': 0.0, 'has_currency_symbol': 0}]\n",
      "[preprocess] Label distribution (current df):\n",
      "{0: 196000, 1: 126798}\n",
      "[preprocess] Completed. Output shape: (322798, 12)\n",
      "[preprocess] Starting preprocess on df with shape (69171, 2)\n",
      "[preprocess] Cleaned text (sample):\n",
      "['The computer plg2 is being replaced with a new machine. We are trying to clean up old accounts that do not have to moved to the new machines. Are you using these accounts or can they be removed? If you still need these accounts, they can be moved to the new computer. plragde:x:101:301:Prabhakar Ragde,DC1314,4660,519-747-9745,82 Ridgeview Cresc. Wloo:/u2/plragde:/xhbin/tcsh nishi:x:137:301:Naomi Nishimura:/u2/nishi:/xhbin/tcsh jfbuss:x:157:301:Jonathan Buss,DC 2113,x4428,(519)747-2771,#506-300 Keats Way, Waterloo, N2L 6E6:/u2/jfbuss:/xhbin/tcsh theorize:x:120:301:CS Theory Group:/u2/theorize:/bin/csh', 'it is going to be huge target sym utevcurrent escapenumber escapenumber escapenumber day target price escapenumberaction strong buy hold bullish profit guaranted escapenumber see bullish news online right now theescapenumber call broker', \"I'm not sure if you have heard our fun news yet, but Rob and I are moving to Boston at the first of the year!! I will be working remotely for HBK while looking for something new up there. Rob is continuing his career with Fidelity, though in a much more convenient location. The only downside is he will actually have to go into the office every day! We would love to see everybody before we leave, though it may not be possible with the holidays. I'll be back in Dallas once a week in January and February, so we could plan to get together during one of those trips. But before that time, Rob and I will be with a group of friends from HBK at Martini Ranch on Thursday, December 14th right after work. I'm sure we'll be there late, given my love of martinis, so please stop by if you can. Regardless, I really want to stay in touch with my Texas friends, so make sure and send me your updated contact information if anything has changed in the last year. Please pass this on to people who I may have inadvertently missed or I did not have their current email address. Cheers, Heather Robertson P.S. By the way, our house is on the market right now, so if you know of anyone looking, please tell them about it. The address is 6815 Vivian Avenue, Dallas, 75223 and can be seen on <URL>\"]\n",
      "[preprocess] Extracting features...\n",
      "[preprocess] Feature summary (first 5 rows):\n",
      "[{'_clean_text': 'The computer plg2 is being replaced with a new machine. We are trying to clean up old accounts that do not have to moved to the new machines. Are you using these accounts or can they be removed? If you still need these accounts, they can be moved to the new computer. plragde:x:101:301:Prabhakar Ragde,DC1314,4660,519-747-9745,82 Ridgeview Cresc. Wloo:/u2/plragde:/xhbin/tcsh nishi:x:137:301:Naomi Nishimura:/u2/nishi:/xhbin/tcsh jfbuss:x:157:301:Jonathan Buss,DC 2113,x4428,(519)747-2771,#506-300 Keats Way, Waterloo, N2L 6E6:/u2/jfbuss:/xhbin/tcsh theorize:x:120:301:CS Theory Group:/u2/theorize:/bin/csh', 'text_len': 606, 'num_words': 70, 'num_links': 0, 'num_uppercase_words': 1, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 4.3125, 'digit_ratio': 0.1254125412541254, 'has_currency_symbol': 0}, {'_clean_text': 'it is going to be huge target sym utevcurrent escapenumber escapenumber escapenumber day target price escapenumberaction strong buy hold bullish profit guaranted escapenumber see bullish news online right now theescapenumber call broker', 'text_len': 236, 'num_words': 32, 'num_links': 0, 'num_uppercase_words': 0, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 6.40625, 'digit_ratio': 0.0, 'has_currency_symbol': 0}, {'_clean_text': \"I'm not sure if you have heard our fun news yet, but Rob and I are moving to Boston at the first of the year!! I will be working remotely for HBK while looking for something new up there. Rob is continuing his career with Fidelity, though in a much more convenient location. The only downside is he will actually have to go into the office every day! We would love to see everybody before we leave, though it may not be possible with the holidays. I'll be back in Dallas once a week in January and February, so we could plan to get together during one of those trips. But before that time, Rob and I will be with a group of friends from HBK at Martini Ranch on Thursday, December 14th right after work. I'm sure we'll be there late, given my love of martinis, so please stop by if you can. Regardless, I really want to stay in touch with my Texas friends, so make sure and send me your updated contact information if anything has changed in the last year. Please pass this on to people who I may have inadvertently missed or I did not have their current email address. Cheers, Heather Robertson P.S. By the way, our house is on the market right now, so if you know of anyone looking, please tell them about it. The address is 6815 Vivian Avenue, Dallas, 75223 and can be seen on <URL>\", 'text_len': 1284, 'num_words': 246, 'num_links': 1, 'num_uppercase_words': 4, 'num_exclamations': 3, 'num_spam_keywords': 0, 'avg_word_length': 3.9919678714859437, 'digit_ratio': 0.008566978193146417, 'has_currency_symbol': 0}, {'_clean_text': 'Per Jason Peters, we are not ready to executed the Guaranty for Florida Power & Light and DLJ International Capital. Jason needs to provide the counterparties with a copy of your mark-ups and comments for each Guaranty so that they can review our changes. If you have any questions, please feel free to contact Sara Shackleton at x-35620 or me at x-39811. Samantha M. Boyd Sr. Legal Specialist Enron North America, Corp. 1400 Smith, EB3802A Houston, TX 77002 Phone: (713) 853-9188 Fax: (713) 646-3490 email: <EMAIL> Samantha Boyd/NA/Enron on 08/15/2000 04:18 PM Sara Shackleton@ECT 08/15/2000 03:41 PM To: Samantha Boyd/NA/Enron@Enron cc: Subject: Florida Power & Light FYI Sara Shackleton/HOU/ECT on 08/15/2000 03:41 PM \"JASON PETERS\" 08/15/2000 03:06 PM To: cc: Subject: Florida Power & Light We\\'re not ready to execute. We are responding to the latest round of comments. Thanks, Jason', 'text_len': 887, 'num_words': 141, 'num_links': 0, 'num_uppercase_words': 12, 'num_exclamations': 0, 'num_spam_keywords': 1, 'avg_word_length': 4.477611940298507, 'digit_ratio': 0.10259301014656144, 'has_currency_symbol': 0}, {'_clean_text': 'Exactly like original. Ravishing Bvlgari watches at Replica Classics Find the best replica rolex, Jewelry etc. <URL>', 'text_len': 116, 'num_words': 17, 'num_links': 1, 'num_uppercase_words': 1, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 5.588235294117647, 'digit_ratio': 0.0, 'has_currency_symbol': 0}]\n",
      "[preprocess] Label distribution (current df):\n",
      "{0: 42000, 1: 27171}\n",
      "[preprocess] Completed. Output shape: (69171, 12)\n",
      "[preprocess] Starting preprocess on df with shape (69171, 2)\n",
      "[preprocess] Cleaned text (sample):\n",
      "['íáóóï×ùå òåëìáíîùå òáóóùìëé × óåôé éîôåòîåô óëïòïóôø íáëóéíáìøîïå ëáþåóô×ï ìàâùå æïòíù ïðìáôù ûéòïëéê ×ùâïò áäòåóîùè âáú íïóë×á ðéôåò òïóóéñ óîç äåìï×áñ íïóë×á escapenumber escapenumber áäò escapenumber ò äåìï×áñ òïóóéñ escapenumber escapenumber áäò escapenumber ò äåìï×ïê ðéôåò escapenumber escapenumber áäò escapenumber ò þáóôîáñ íïóë×á escapenumber escapenumber escapenumber áäò escapenumber ò þáóôîáñ òïóóéñ escapenumber escapenumber escapenumber áäò escapenumber ò äåìï×áñ õëòáéîá escapenumber escapenumber áäò escapenumber þáóôîáñ õëòáéîá escapenumber escapenumber áäò escapenumber äåìï×áñ âåìïòõóóéñ escapenumber escapenumber áäò escapenumber ò ú×ïîéôå é úáëáúù×áêôå escapenumber escapenumber escapenumber îáûé íåîåäöåòù ïô×åôñô îá ×óå éîôåòåóõàýéå ÷áó ×ïðòïóù ëïíðøàôåò õþáóôîéë óáíïïþéýåîéå õäé×ìñàóø áìøâéïîá âáúï×õà éúäáôåìà ëòé×õà ôïòçï×õà', \"Hi, I would like to know if someone could clarify a question regarding license for redistribution of raw data in the datasets package. If possible, I would like to reuse those data in a package for scipy ( <URL> which is under a BSD license. The package datasets itself is under GPL, but I don't know if this covers the actual data too ? cheers, David <EMAIL> mailing list <URL> PLEASE do read the posting guide <URL> and provide commented, minimal, self-contained, reproducible code.\", 'All: PLEASE NOTE THE FOLLOWING BASED UPON MY UNDERSTANDING OF THE PARTIES TO THIS PREPAY (1) The Toronto Dominion (Texas) master agreement with ECT is a restricted master. It was negotiated for a prepay several years ago and is not necessarily market (or ENA) standard. It should be reviewed immediately by legal and credit. (2) The Morgan Stanley Capital Group master agreement is very old. IT IS NOT AN ISDA . It is a 1992 Master Energy Price Swap Agreement (limited to simple transactions and commodities). The terminology is not even ISDA terminology. This master has been on the credit/legal radar screen for upgrading to an ISDA. Rod: You need to take a look at the Toronto Dominion (Texas) master agreement now and we should discuss. Treasa: Will there be a rate swap? Under which master(s)?']\n",
      "[preprocess] Extracting features...\n",
      "[preprocess] Feature summary (first 5 rows):\n",
      "[{'_clean_text': 'íáóóï×ùå òåëìáíîùå òáóóùìëé × óåôé éîôåòîåô óëïòïóôø íáëóéíáìøîïå ëáþåóô×ï ìàâùå æïòíù ïðìáôù ûéòïëéê ×ùâïò áäòåóîùè âáú íïóë×á ðéôåò òïóóéñ óîç äåìï×áñ íïóë×á escapenumber escapenumber áäò escapenumber ò äåìï×áñ òïóóéñ escapenumber escapenumber áäò escapenumber ò äåìï×ïê ðéôåò escapenumber escapenumber áäò escapenumber ò þáóôîáñ íïóë×á escapenumber escapenumber escapenumber áäò escapenumber ò þáóôîáñ òïóóéñ escapenumber escapenumber escapenumber áäò escapenumber ò äåìï×áñ õëòáéîá escapenumber escapenumber áäò escapenumber þáóôîáñ õëòáéîá escapenumber escapenumber áäò escapenumber äåìï×áñ âåìïòõóóéñ escapenumber escapenumber áäò escapenumber ò ú×ïîéôå é úáëáúù×áêôå escapenumber escapenumber escapenumber îáûé íåîåäöåòù ïô×åôñô îá ×óå éîôåòåóõàýéå ÷áó ×ïðòïóù ëïíðøàôåò õþáóôîéë óáíïïþéýåîéå õäé×ìñàóø áìøâéïîá âáúï×õà éúäáôåìà ëòé×õà ôïòçï×õà', 'text_len': 851, 'num_words': 99, 'num_links': 0, 'num_uppercase_words': 0, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 6.356521739130435, 'digit_ratio': 0.0, 'has_currency_symbol': 0}, {'_clean_text': \"Hi, I would like to know if someone could clarify a question regarding license for redistribution of raw data in the datasets package. If possible, I would like to reuse those data in a package for scipy ( <URL> which is under a BSD license. The package datasets itself is under GPL, but I don't know if this covers the actual data too ? cheers, David <EMAIL> mailing list <URL> PLEASE do read the posting guide <URL> and provide commented, minimal, self-contained, reproducible code.\", 'text_len': 484, 'num_words': 84, 'num_links': 3, 'num_uppercase_words': 7, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 4.511904761904762, 'digit_ratio': 0.0, 'has_currency_symbol': 0}, {'_clean_text': 'All: PLEASE NOTE THE FOLLOWING BASED UPON MY UNDERSTANDING OF THE PARTIES TO THIS PREPAY (1) The Toronto Dominion (Texas) master agreement with ECT is a restricted master. It was negotiated for a prepay several years ago and is not necessarily market (or ENA) standard. It should be reviewed immediately by legal and credit. (2) The Morgan Stanley Capital Group master agreement is very old. IT IS NOT AN ISDA . It is a 1992 Master Energy Price Swap Agreement (limited to simple transactions and commodities). The terminology is not even ISDA terminology. This master has been on the credit/legal radar screen for upgrading to an ISDA. Rod: You need to take a look at the Toronto Dominion (Texas) master agreement now and we should discuss. Treasa: Will there be a rate swap? Under which master(s)?', 'text_len': 798, 'num_words': 136, 'num_links': 0, 'num_uppercase_words': 23, 'num_exclamations': 0, 'num_spam_keywords': 2, 'avg_word_length': 4.686567164179104, 'digit_ratio': 0.007518796992481203, 'has_currency_symbol': 0}, {'_clean_text': 'bc fescapenumber cescapenumber descapenumber bescapenumber dd ba descapenumber bf ec cescapenumber cc bf fescapenumber bescapenumber bd bescapenumber eescapenumber bb eescapenumber bf eb bescapenumber bescapenumber cescapenumber ba cescapenumber cc bf eb ba ce cescapenumber bescapenumber bescapenumber eescapenumber bescapenumber bescapenumber bb eescapenumber cescapenumber fescapenumber cescapenumber cc ca cescapenumber cc ba bescapenumber bescapenumber eescapenumber bf aescapenumber bescapenumber eescapenumber cescapenumber ac bescapenumber af cescapenumber cf bc bc bf eescapenumber', 'text_len': 590, 'num_words': 65, 'num_links': 0, 'num_uppercase_words': 0, 'num_exclamations': 0, 'num_spam_keywords': 0, 'avg_word_length': 8.092307692307692, 'digit_ratio': 0.0, 'has_currency_symbol': 0}, {'_clean_text': '[IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] Danny & Nicolec Worthingtonc Staffordc J. Ferrarc [IMAGE] [IMAGE] R & M Richardsc Embellished Dress - $79.00-$89.00 Staffordc 2-button Hopsack Blazer - $79.99-$109.99 Staffordc Microfiber Trousers - $29.99-$39.99 Bellini Straw Hat - $95.00 Staffordc Short Sleeve Wrinkle-free Solid Oxford Shirt - price varies [IMAGE] [IMAGE] Shop the way you want... If you prefer to shop from our catalogs, call 1-800-222-6161. You can choose to ship your order to your home, office or to a JCPenney Catalog Desk near you! [IMAGE] We sent you this message because our records show you asked for e-mail announcements. We practice permission marketing. If you want to remove yourself from the list, click here. , 2000 J.C. Penney Company, Inc. and/or JCP ECommerce L.P., 6501 Legacy Drive, Plano, Texas, U.S.A. All rights reserved.', 'text_len': 1070, 'num_words': 160, 'num_links': 0, 'num_uppercase_words': 42, 'num_exclamations': 1, 'num_spam_keywords': 2, 'avg_word_length': 4.802631578947368, 'digit_ratio': 0.04579439252336449, 'has_currency_symbol': 1}]\n",
      "[preprocess] Label distribution (current df):\n",
      "{0: 42000, 1: 27171}\n",
      "[preprocess] Completed. Output shape: (69171, 12)\n",
      ">>> Sanity checks:\n",
      "Train shape: (322798, 12) Label dist: {0: 0.6071908747885675, 1: 0.3928091252114325}\n",
      "Val   shape: (69171, 12) Label dist: {0: 0.6071908747885675, 1: 0.3928091252114325}\n",
      "Test  shape: (69171, 12) Label dist: {0: 0.6071908747885675, 1: 0.3928091252114325}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                  message  label  \\\n",
       " 44701   [THE BULL REPORT! !! Butt sym: CHVCCurrent: $ ...      1   \n",
       " 111842  Confidential I will send you updates ---------...      0   \n",
       " 421894  \\nHealthMedsCustomerSupport\\nhttp://e6raeq.blu...      1   \n",
       " 65390   On mahonia margining, I spoke with mike garber...      0   \n",
       " 144759   begin pgp signed message hash shaescapenumber...      0   \n",
       " ...                                                   ...    ...   \n",
       " 150042  this is amazing stuff add some inches fast saf...      1   \n",
       " 76597   I am sure you all will be reviewing the approp...      0   \n",
       " 357858  ----------------------  Darron C Giron/HOU/ECT...      0   \n",
       " 401535  carlos ,\\ni created the following deal for 4 /...      0   \n",
       " 312671  are you ready ?\\nwe are accepting mortgage req...      1   \n",
       " \n",
       "                                               _clean_text  text_len  \\\n",
       " 44701   ['THE BULL REPORT! !! Butt sym: CHVCCurrent: $...       227   \n",
       " 111842  Confidential I will send you updates Hunter S ...       168   \n",
       " 421894                    HealthMedsCustomerSupport <URL>        31   \n",
       " 65390   On mahonia margining, I spoke with mike garber...       366   \n",
       " 144759  begin pgp signed message hash shaescapenumber ...       651   \n",
       " ...                                                   ...       ...   \n",
       " 150042  this is amazing stuff add some inches fast saf...       190   \n",
       " 76597   I am sure you all will be reviewing the approp...       543   \n",
       " 357858  Darron C Giron/HOU/ECT on 10/17/2000 03:20 PM ...       245   \n",
       " 401535  carlos , i created the following deal for 4 / ...       176   \n",
       " 312671  are you ready ? we are accepting mortgage requ...       373   \n",
       " \n",
       "         num_words  num_links  num_uppercase_words  num_exclamations  \\\n",
       " 44701          40          0                    4                 4   \n",
       " 111842         26          0                    3                 0   \n",
       " 421894          2          1                    1                 0   \n",
       " 65390          61          1                    1                 0   \n",
       " 144759        105          0                    0                 0   \n",
       " ...           ...        ...                  ...               ...   \n",
       " 150042         33          0                    0                 0   \n",
       " 76597          91          0                    0                 0   \n",
       " 357858         44          0                    2                 0   \n",
       " 401535         45          0                    0                 0   \n",
       " 312671         88          0                    0                 1   \n",
       " \n",
       "         num_spam_keywords  avg_word_length  digit_ratio  has_currency_symbol  \n",
       " 44701                   0         4.939394     0.017621                    1  \n",
       " 111842                  0         4.444444     0.071429                    0  \n",
       " 421894                  0        14.000000     0.000000                    0  \n",
       " 65390                   1         4.966667     0.000000                    0  \n",
       " 144759                  1         5.150943     0.000000                    0  \n",
       " ...                   ...              ...          ...                  ...  \n",
       " 150042                  0         4.787879     0.000000                    0  \n",
       " 76597                   1         4.626374     0.009208                    1  \n",
       " 357858                  0         4.022727     0.048980                    0  \n",
       " 401535                  1         5.000000     0.147727                    1  \n",
       " 312671                  1         4.193548     0.000000                    0  \n",
       " \n",
       " [322798 rows x 12 columns],\n",
       "                                                   message  label  \\\n",
       " 126651  The computer plg2 is being replaced with a new...      1   \n",
       " 343831  it is going to be huge target sym utevcurrent ...      1   \n",
       " 9566    I'm not sure if you have heard our fun news ye...      0   \n",
       " 454178  Per Jason Peters, we are not ready to executed...      0   \n",
       " 336644  \\nExactly like original. \\n\\nRavishing Bvlgari...      1   \n",
       " ...                                                   ...    ...   \n",
       " 29550                 is speaking to Mike McConnell today      0   \n",
       " 455316  Jan Cobden Enron Transportation Services Compa...      0   \n",
       " 75420   Audrey D. Robertson Transwestern Pipeline Comp...      0   \n",
       " 38131   [welcome our penis xtrasize pills volition exp...      1   \n",
       " 337338  on tue jun escapenumber escapenumber at escape...      0   \n",
       " \n",
       "                                               _clean_text  text_len  \\\n",
       " 126651  The computer plg2 is being replaced with a new...       606   \n",
       " 343831  it is going to be huge target sym utevcurrent ...       236   \n",
       " 9566    I'm not sure if you have heard our fun news ye...      1284   \n",
       " 454178  Per Jason Peters, we are not ready to executed...       887   \n",
       " 336644  Exactly like original. Ravishing Bvlgari watch...       116   \n",
       " ...                                                   ...       ...   \n",
       " 29550                 is speaking to Mike McConnell today        35   \n",
       " 455316  Jan Cobden Enron Transportation Services Compa...       878   \n",
       " 75420   Audrey D. Robertson Transwestern Pipeline Comp...      1112   \n",
       " 38131   ['welcome our penis xtrasize pills volition ex...       671   \n",
       " 337338  on tue jun escapenumber escapenumber at escape...       278   \n",
       " \n",
       "         num_words  num_links  num_uppercase_words  num_exclamations  \\\n",
       " 126651         70          0                    1                 0   \n",
       " 343831         32          0                    0                 0   \n",
       " 9566          246          1                    4                 3   \n",
       " 454178        141          0                   12                 0   \n",
       " 336644         17          1                    1                 0   \n",
       " ...           ...        ...                  ...               ...   \n",
       " 29550           6          0                    0                 0   \n",
       " 455316        137          0                   37                 0   \n",
       " 75420         155          0                  111                 0   \n",
       " 38131         108          0                    0                 0   \n",
       " 337338         43          0                    0                 0   \n",
       " \n",
       "         num_spam_keywords  avg_word_length  digit_ratio  has_currency_symbol  \n",
       " 126651                  0         4.312500     0.125413                    0  \n",
       " 343831                  0         6.406250     0.000000                    0  \n",
       " 9566                    0         3.991968     0.008567                    0  \n",
       " 454178                  1         4.477612     0.102593                    0  \n",
       " 336644                  0         5.588235     0.000000                    0  \n",
       " ...                   ...              ...          ...                  ...  \n",
       " 29550                   0         5.000000     0.000000                    0  \n",
       " 455316                  0         5.098485     0.048975                    0  \n",
       " 75420                   1         5.263158     0.119604                    0  \n",
       " 38131                   2         5.185185     0.000000                    0  \n",
       " 337338                  0         5.340909     0.000000                    0  \n",
       " \n",
       " [69171 rows x 12 columns],\n",
       "                                                   message  label  \\\n",
       " 235738  íáóóï×ùå òåëìáíîùå òáóóùìëé × óåôé éîôåòîåô óë...      1   \n",
       " 245748  Hi,\\n\\n    I would like to know if someone cou...      0   \n",
       " 172671  All: PLEASE NOTE THE FOLLOWING BASED UPON MY U...      0   \n",
       " 19777   bc fescapenumber cescapenumber descapenumber b...      1   \n",
       " 24012   [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE...      0   \n",
       " ...                                                   ...    ...   \n",
       " 263313  Howdy all! Just an FYI: If you are interested ...      0   \n",
       " 89870   Please add your insert to this version. Thanks...      0   \n",
       " 140823  i cannot go to denver the last week of may b/c...      0   \n",
       " 458202  Date: Wed, 19 Sep 2001 16:21:45 -0500 wfubmc.e...      0   \n",
       " 313844  tell fair kent heid hebrew word themy aunt exa...      1   \n",
       " \n",
       "                                               _clean_text  text_len  \\\n",
       " 235738  íáóóï×ùå òåëìáíîùå òáóóùìëé × óåôé éîôåòîåô óë...       851   \n",
       " 245748  Hi, I would like to know if someone could clar...       484   \n",
       " 172671  All: PLEASE NOTE THE FOLLOWING BASED UPON MY U...       798   \n",
       " 19777   bc fescapenumber cescapenumber descapenumber b...       590   \n",
       " 24012   [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE] [IMAGE...      1070   \n",
       " ...                                                   ...       ...   \n",
       " 263313  Howdy all! Just an FYI: If you are interested ...       347   \n",
       " 89870   Please add your insert to this version. Thanks...       471   \n",
       " 140823  i cannot go to denver the last week of may b/c...       416   \n",
       " 458202  Date: Wed, 19 Sep 2001 16:21:45 -0500 wfubmc.e...      2715   \n",
       " 313844  tell fair kent heid hebrew word themy aunt exa...       200   \n",
       " \n",
       "         num_words  num_links  num_uppercase_words  num_exclamations  \\\n",
       " 235738         99          0                    0                 0   \n",
       " 245748         84          3                    7                 0   \n",
       " 172671        136          0                   23                 0   \n",
       " 19777          65          0                    0                 0   \n",
       " 24012         160          0                   42                 1   \n",
       " ...           ...        ...                  ...               ...   \n",
       " 263313         62          0                    6                 2   \n",
       " 89870          73          0                    4                 0   \n",
       " 140823         92          0                    0                 0   \n",
       " 458202        497          0                   14                 3   \n",
       " 313844         25          0                    0                 0   \n",
       " \n",
       "         num_spam_keywords  avg_word_length  digit_ratio  has_currency_symbol  \n",
       " 235738                  0         6.356522     0.000000                    0  \n",
       " 245748                  0         4.511905     0.000000                    0  \n",
       " 172671                  2         4.686567     0.007519                    0  \n",
       " 19777                   0         8.092308     0.000000                    0  \n",
       " 24012                   2         4.802632     0.045794                    1  \n",
       " ...                   ...              ...          ...                  ...  \n",
       " 263313                  0         4.092308     0.011527                    0  \n",
       " 89870                   0         4.493333     0.076433                    0  \n",
       " 140823                  1         3.397849     0.000000                    0  \n",
       " 458202                  1         5.121581     0.018416                    0  \n",
       " 313844                  0         7.040000     0.000000                    0  \n",
       " \n",
       " [69171 rows x 12 columns])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: preprocessing + feature engineering\n",
    "# Requirements: pandas, numpy, regex (stdlib), scikit-learn (optional)\n",
    "# Install if needed:\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# --- Config ---\n",
    "text_col = \"message\"   # change to your actual column name if different\n",
    "label_col = \"label\"\n",
    "VERBOSE = 1  # user requested verbose=1 outputs\n",
    "\n",
    "# --- Helper functions ---\n",
    "def clean_html_and_text(s: str) -> str:\n",
    "    \"\"\"Remove HTML tags, URLs, emails, excessive whitespace; do small signature cut.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    # Remove HTML tags\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    # Replace URLs with a token\n",
    "    s = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" <URL> \", s)\n",
    "    # Replace email addresses\n",
    "    s = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \" <EMAIL> \", s)\n",
    "    # Replace sequences of hyphens/underscores (common in signatures)\n",
    "    s = re.sub(r\"[-_]{2,}\", \" \", s)\n",
    "    # Remove multiple newlines and trim signature-like tails (simple heuristic)\n",
    "    lines = [ln.strip() for ln in s.splitlines() if ln.strip() != \"\"]\n",
    "    # Remove trailing lines that look like 'Regards, Name' or 'Thanks, Name'\n",
    "    if len(lines) > 3:\n",
    "        tail = \" \".join(lines[-3:]).lower()\n",
    "        if any(k in tail for k in [\"regards\", \"thanks\", \"thank you\", \"sincerely\", \"best regards\"]):\n",
    "            lines = lines[:-3]\n",
    "    s = \" \".join(lines)\n",
    "    # Normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Spam-related keyword set (extendable)\n",
    "SPAM_KEYWORDS = {\n",
    "    \"free\",\"win\",\"winner\",\"prize\",\"click\",\"offer\",\"buy now\",\"limited time\",\"urgent\",\n",
    "    \"guarantee\",\"act now\",\"congratulations\",\"claim\",\"money\",\"loan\",\"credit\",\"dear friend\"\n",
    "}\n",
    "\n",
    "def count_uppercase_words(s: str) -> int:\n",
    "    return sum(1 for w in s.split() if w.isupper() and len(w) > 1)\n",
    "\n",
    "def count_exclamations(s: str) -> int:\n",
    "    return s.count(\"!\")\n",
    "\n",
    "def count_links(s: str) -> int:\n",
    "    return len(re.findall(r\"<URL>\", s))  # from cleaning replacement\n",
    "\n",
    "def count_digits_ratio(s: str) -> float:\n",
    "    if len(s) == 0:\n",
    "        return 0.0\n",
    "    digits = sum(c.isdigit() for c in s)\n",
    "    return digits / max(1, len(s))\n",
    "\n",
    "def avg_word_length(s: str) -> float:\n",
    "    words = [w for w in re.findall(r\"\\w+\", s) if not w.isnumeric()]\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return sum(len(w) for w in words) / len(words)\n",
    "\n",
    "def count_spam_keywords(s: str, keywords: set) -> int:\n",
    "    s_low = s.lower()\n",
    "    return sum(1 for kw in keywords if kw in s_low)\n",
    "\n",
    "def contains_currency_symbol(s: str) -> int:\n",
    "    return int(bool(re.search(r\"[\\$£€₹]\", s)))\n",
    "\n",
    "# Main preprocess function\n",
    "def preprocess_df(df: pd.DataFrame,\n",
    "                  text_col: str = \"text\",\n",
    "                  label_col: str = \"label\",\n",
    "                  spam_keywords: set = SPAM_KEYWORDS,\n",
    "                  verbose: int = VERBOSE) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if verbose:\n",
    "        print(f\"[preprocess] Starting preprocess on df with shape {df.shape}\")\n",
    "    # Clean text\n",
    "    df[\"_clean_text\"] = df[text_col].fillna(\"\").astype(str).map(clean_html_and_text)\n",
    "    if verbose:\n",
    "        print(\"[preprocess] Cleaned text (sample):\")\n",
    "        print(df[\"_clean_text\"].head(3).to_list())\n",
    "\n",
    "    # Feature engineering\n",
    "    if verbose:\n",
    "        print(\"[preprocess] Extracting features...\")\n",
    "\n",
    "    df[\"text_len\"] = df[\"_clean_text\"].map(len)\n",
    "    df[\"num_words\"] = df[\"_clean_text\"].map(lambda s: len(s.split()))\n",
    "    df[\"num_links\"] = df[\"_clean_text\"].map(count_links)\n",
    "    df[\"num_uppercase_words\"] = df[\"_clean_text\"].map(count_uppercase_words)\n",
    "    df[\"num_exclamations\"] = df[\"_clean_text\"].map(count_exclamations)\n",
    "    df[\"num_spam_keywords\"] = df[\"_clean_text\"].map(lambda s: count_spam_keywords(s, spam_keywords))\n",
    "    df[\"avg_word_length\"] = df[\"_clean_text\"].map(avg_word_length)\n",
    "    df[\"digit_ratio\"] = df[\"_clean_text\"].map(count_digits_ratio)\n",
    "    df[\"has_currency_symbol\"] = df[\"_clean_text\"].map(contains_currency_symbol)\n",
    "\n",
    "    # Optional: simple URL domain feature extraction (counts of domains)\n",
    "    # If you want domain counts later, keep the original url detection and extract domain names.\n",
    "\n",
    "    # Summarize\n",
    "    if verbose:\n",
    "        print(\"[preprocess] Feature summary (first 5 rows):\")\n",
    "        print(df[[\n",
    "            \"_clean_text\", \"text_len\", \"num_words\", \"num_links\",\n",
    "            \"num_uppercase_words\", \"num_exclamations\", \"num_spam_keywords\",\n",
    "            \"avg_word_length\", \"digit_ratio\", \"has_currency_symbol\"\n",
    "        ]].head(5).to_dict(orient=\"records\"))\n",
    "\n",
    "        # label distribution\n",
    "        if label_col in df.columns:\n",
    "            print(\"[preprocess] Label distribution (current df):\")\n",
    "            print(df[label_col].value_counts(dropna=False).to_dict())\n",
    "\n",
    "        print(f\"[preprocess] Completed. Output shape: {df.shape}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Example usage ---\n",
    "# (Assumes train_df, val_df, test_df exist in the environment)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\">>> Running Step 1: preprocessing on train/val/test with verbose=1\")\n",
    "\n",
    "train_clean = preprocess_df(train_df, text_col=text_col, label_col=label_col, verbose=VERBOSE)\n",
    "val_clean   = preprocess_df(val_df,   text_col=text_col, label_col=label_col, verbose=VERBOSE)\n",
    "test_clean  = preprocess_df(test_df,  text_col=text_col, label_col=label_col, verbose=VERBOSE)\n",
    "\n",
    "# Quick sanity checks and saving (optional)\n",
    "if VERBOSE:\n",
    "    print(\">>> Sanity checks:\")\n",
    "    print(\"Train shape:\", train_clean.shape, \"Label dist:\", train_clean[label_col].value_counts(normalize=True).to_dict())\n",
    "    print(\"Val   shape:\", val_clean.shape,   \"Label dist:\", val_clean[label_col].value_counts(normalize=True).to_dict())\n",
    "    print(\"Test  shape:\", test_clean.shape,  \"Label dist:\", test_clean[label_col].value_counts(normalize=True).to_dict())\n",
    "\n",
    "# Optionally save to disk for later steps (uncomment to use)\n",
    "# train_clean.to_parquet(\"train_clean.parquet\", index=False)\n",
    "# val_clean.to_parquet(\"val_clean.parquet\", index=False)\n",
    "# test_clean.to_parquet(\"test_clean.parquet\", index=False)\n",
    "\n",
    "# Return objects to interactive session (if running as a cell)\n",
    "train_clean, val_clean, test_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10c446fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tokenizers) (1.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: shellingham in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.1.8)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 25.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.22.1\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "48edf2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 4.7/12.0 MB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.0 MB 24.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 23.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 566.1/566.1 kB 15.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, transformers\n",
      "\n",
      "  Attempting uninstall: huggingface-hub\n",
      "\n",
      "    Found existing installation: huggingface-hub 1.0.1\n",
      "\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "    Uninstalling huggingface-hub-1.0.1:\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "      Successfully uninstalled huggingface-hub-1.0.1\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [huggingface-hub]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.36.0 safetensors-0.6.2 transformers-4.57.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "675d5771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktokenNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
      "Downloading tiktoken-0.12.0-cp313-cp313-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.1 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 786.4/879.1 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 879.1/879.1 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bb8e336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ByteLevel BPE tokenizer with vocab_size=30000 ...\n",
      "Tokenizer trained and saved to email_tokenizer_bpe\n",
      "Vocab size: 30000\n",
      "[prepare] Tokenizer ready. Creating MLM examples for train/val/test (this may use memory).\n",
      "[mlm] Tokenizing 322798 texts with max_length=256 and mlm_prob=0.15\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 2: Train tokenizer + prepare MLM masked examples\n",
    "# Requirements: tokenizers, transformers, numpy, pandas\n",
    "# pip install tokenizers transformers numpy pandas --upgrade\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizerFast\n",
    "from typing import List, Tuple\n",
    "\n",
    "VERBOSE = 1  # user requested verbose=1\n",
    "\n",
    "# --- Config ---\n",
    "tokenizer_dir = \"email_tokenizer_bpe\"\n",
    "vocab_size = 30_000      # reasonable for email domain; adjust if you want smaller/larger\n",
    "min_frequency = 2\n",
    "max_length = 256         # sequence length for MLM examples\n",
    "mlm_probability = 0.15   # standard 15% masking\n",
    "\n",
    "# --- Train ByteLevel BPE tokenizer ---\n",
    "def train_bytelevel_bpe_tokenizer(train_texts, save_dir, vocab_size=30000, verbose=1):\n",
    "    import os\n",
    "    from tokenizers import ByteLevelBPETokenizer\n",
    "    from transformers import RobertaTokenizerFast  # ✅ moved here first\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training ByteLevel BPE tokenizer with vocab_size={vocab_size} ...\")\n",
    "\n",
    "    # Train tokenizer on provided texts\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train_from_iterator(\n",
    "        train_texts,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=2,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "    )\n",
    "\n",
    "    # Save to directory\n",
    "    tokenizer.save_model(save_dir)\n",
    "\n",
    "    # Load fast version directly from the saved folder\n",
    "    tokenizer_fast = RobertaTokenizerFast.from_pretrained(save_dir)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Tokenizer trained and saved to {save_dir}\")\n",
    "        print(f\"Vocab size: {tokenizer_fast.vocab_size}\")\n",
    "\n",
    "    return tokenizer_fast\n",
    "\n",
    "# --- MLM masking utility (BERT-style) ---\n",
    "def create_mlm_samples(texts: List[str],\n",
    "                       tokenizer,\n",
    "                       max_length: int = max_length,\n",
    "                       mlm_prob: float = mlm_probability,\n",
    "                       verbose: int = VERBOSE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a list of raw texts and a tokenizer (PreTrainedTokenizerFast),\n",
    "    returns numpy arrays (input_ids, labels) ready for MLM training:\n",
    "      - input_ids: token ids with [MASK]/random/unchanged applied\n",
    "      - labels: original token ids for masked positions, -100 elsewhere\n",
    "    Sequences are truncated/padded to max_length.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[mlm] Tokenizing {len(texts)} texts with max_length={max_length} and mlm_prob={mlm_prob}\")\n",
    "\n",
    "    encodings = tokenizer(texts, truncation=True, padding=\"max_length\",\n",
    "                          max_length=max_length, return_attention_mask=False)\n",
    "    input_ids = np.array(encodings[\"input_ids\"], dtype=np.int32)\n",
    "    vocab_size_local = tokenizer.vocab_size\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    unk_token_id = tokenizer.unk_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    labels = np.full_like(input_ids, fill_value=-100)  # default ignore index\n",
    "\n",
    "    rng = np.random.RandomState(42)  # deterministic for reproducibility\n",
    "\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        ids = input_ids[i]\n",
    "        # create candidate mask positions (exclude special tokens: bos/eos/pad)\n",
    "        special_ids = {tokenizer.bos_token_id, tokenizer.eos_token_id, pad_token_id}\n",
    "        candidate_positions = [pos for pos, tok in enumerate(ids)\n",
    "                               if tok not in special_ids]\n",
    "        # skip empty\n",
    "        if len(candidate_positions) == 0:\n",
    "            continue\n",
    "\n",
    "        num_to_mask = max(1, int(round(len(candidate_positions) * mlm_prob)))\n",
    "        mask_positions = rng.choice(candidate_positions, size=num_to_mask, replace=False)\n",
    "\n",
    "        for pos in mask_positions:\n",
    "            orig_token = ids[pos]\n",
    "            labels[i, pos] = orig_token  # we predict this token\n",
    "\n",
    "            prob = rng.rand()\n",
    "            if prob < 0.8:\n",
    "                # 80% -> replace with [MASK]\n",
    "                ids[pos] = mask_token_id\n",
    "            elif prob < 0.9:\n",
    "                # 10% -> replace with random token\n",
    "                rand_tok = rng.randint(0, vocab_size_local)\n",
    "                # avoid special tokens for random (simple attempt)\n",
    "                while rand_tok in (tokenizer.bos_token_id, tokenizer.eos_token_id, pad_token_id, unk_token_id):\n",
    "                    rand_tok = rng.randint(0, vocab_size_local)\n",
    "                ids[pos] = int(rand_tok)\n",
    "            else:\n",
    "                # 10% -> keep original token (no change)\n",
    "                pass\n",
    "\n",
    "        # update input_ids row\n",
    "        input_ids[i] = ids\n",
    "\n",
    "        if verbose and (i + 1) % 10000 == 0:\n",
    "            print(f\"[mlm] processed {i+1} / {input_ids.shape[0]}\")\n",
    "\n",
    "    if verbose:\n",
    "        total_masked = np.sum(labels != -100)\n",
    "        print(f\"[mlm] Masking complete. Total masked tokens: {int(total_masked)}\")\n",
    "    return input_ids, labels\n",
    "\n",
    "# --- Putting it together ---\n",
    "def prepare_tokenizer_and_mlm(train_df, val_df=None, test_df=None,\n",
    "                             text_col=\"_clean_text\",\n",
    "                             save_dir=tokenizer_dir,\n",
    "                             verbose=VERBOSE):\n",
    "    # Train tokenizer on train texts\n",
    "    train_texts = train_df[text_col].astype(str).tolist()\n",
    "    tokenizer = train_bytelevel_bpe_tokenizer(train_texts, save_dir=save_dir, verbose=verbose)\n",
    "\n",
    "    # Prepare small samples for debugging & then full datasets\n",
    "    if verbose:\n",
    "        print(\"[prepare] Tokenizer ready. Creating MLM examples for train/val/test (this may use memory).\")\n",
    "\n",
    "    train_inputs, train_labels = create_mlm_samples(train_texts, tokenizer, max_length=max_length, mlm_prob=mlm_probability, verbose=verbose)\n",
    "\n",
    "    val_inputs, val_labels = (None, None)\n",
    "    test_inputs, test_labels = (None, None)\n",
    "    if val_df is not None:\n",
    "        val_texts = val_df[text_col].astype(str).tolist()\n",
    "        val_inputs, val_labels = create_mlm_samples(val_texts, tokenizer, max_length=max_length, mlm_prob=mlm_probability, verbose=verbose)\n",
    "    if test_df is not None:\n",
    "        test_texts = test_df[text_col].astype(str).tolist()\n",
    "        test_inputs, test_labels = create_mlm_samples(test_texts, tokenizer, max_length=max_length, mlm_prob=mlm_probability, verbose=verbose)\n",
    "\n",
    "    # Optionally save arrays to disk (uncomment to use)\n",
    "    np.savez_compressed(os.path.join(save_dir, \"mlm_train.npz\"), input_ids=train_inputs, labels=train_labels)\n",
    "    if val_inputs is not None:\n",
    "        np.savez_compressed(os.path.join(save_dir, \"mlm_val.npz\"), input_ids=val_inputs, labels=val_labels)\n",
    "    if test_inputs is not None:\n",
    "        np.savez_compressed(os.path.join(save_dir, \"mlm_test.npz\"), input_ids=test_inputs, labels=test_labels)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[prepare] Saved MLM arrays to {save_dir}\")\n",
    "        print(\"  train shape:\", train_inputs.shape, train_labels.shape)\n",
    "        if val_inputs is not None:\n",
    "            print(\"  val   shape:\", val_inputs.shape, val_labels.shape)\n",
    "        if test_inputs is not None:\n",
    "            print(\"  test  shape:\", test_inputs.shape, test_labels.shape)\n",
    "\n",
    "    return tokenizer, (train_inputs, train_labels), (val_inputs, val_labels), (test_inputs, test_labels)\n",
    "\n",
    "\n",
    "# --- Run prepare (assumes train_clean, val_clean, test_clean available) ---\n",
    "# Warning: this will write a corpus file and may take a few minutes depending on dataset size.\n",
    "tokenizer, train_mlm, val_mlm, test_mlm = prepare_tokenizer_and_mlm(train_clean, val_clean, test_clean, text_col=\"_clean_text\", verbose=VERBOSE)\n",
    "\n",
    "# `tokenizer` is a transformers tokenizer (RobertaTokenizerFast)\n",
    "# `train_mlm` is tuple (input_ids, labels) numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Cleaning utilities ----------\n",
    "URL_RE = re.compile(r'(https?://\\S+|www\\.\\S+)')\n",
    "EMAIL_RE = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+')\n",
    "HTML_TAG_RE = re.compile(r'<[^>]+>')\n",
    "MULTI_SPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text, replace_urls_with=\"<URL>\", replace_emails_with=\"<EMAIL>\"):\n",
    "    \"\"\"\n",
    "    Basic, robust cleaning:\n",
    "    - Ensure string type\n",
    "    - Remove HTML tags\n",
    "    - Replace URLs and email addresses with special tokens\n",
    "    - Lowercase\n",
    "    - Normalize whitespace\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = HTML_TAG_RE.sub(' ', s)                    # remove html tags\n",
    "    s = URL_RE.sub(' ' + replace_urls_with + ' ', s)  # replace urls\n",
    "    s = EMAIL_RE.sub(' ' + replace_emails_with + ' ', s)  # replace emails\n",
    "    s = s.lower()\n",
    "    s = MULTI_SPACE_RE.sub(' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# ---------- 2) Preprocess DataFrame ----------\n",
    "def preprocess_df(dfe,\n",
    "                  combine_subject_body=True,\n",
    "                  subject_col='subject',\n",
    "                  body_col='body',\n",
    "                  urls_col='urls',\n",
    "                  label_col='label'):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with a 'texSW   `t' column and 'label' column ready for tokenization.\n",
    "    combine_subject_body: True => create text = \"<SUBJ> {subject} <BODY> {body}\"\n",
    "    If you want separate handling later (e.g., two-channel model), set False.\n",
    "    \"\"\"\n",
    "    df = train_df.copy()\n",
    "    # Clean each column individually\n",
    "    df['clean_subject'] = df[subject_col].apply(clean_text)\n",
    "    df['clean_body'] = df[body_col].apply(clean_text)\n",
    "\n",
    "    # If urls column exists, we can also normalize or append it as a token\n",
    "    if urls_col in df.columns:\n",
    "        df['clean_urls'] = df[urls_col].fillna('').astype(str).apply(lambda x: (' <URLS> ' + x) if x.strip() else '')\n",
    "        # Optionally you could `extract domain` or count urls instead of embedding them; here we keep them as extra text.\n",
    "    else:\n",
    "        df['clean_urls'] = ''\n",
    "\n",
    "    if combine_subject_body:\n",
    "        # add explicit segment tokens so model can learn different patterns in subject vs body\n",
    "        df['text'] = (\"<SUBJ> \" + df['clean_subject'] + \" <BODY> \" + df['clean_body'] + \" \" + df['clean_urls']).str.strip()\n",
    "    else:\n",
    "        # keep subject and body separate columns (useful for 2-input models)\n",
    "        df['text_subject'] = df['clean_subject']\n",
    "        df['text_body'] = df['clean_body']\n",
    "        df['text_urls'] = df['clean_urls']\n",
    "        # You can still create a combined field for baseline:\n",
    "        df['text'] = (\"<SUBJ> \" + df['clean_subject'] + \" <BODY> \" + df['clean_body']).str.strip()\n",
    "\n",
    "    # Ensure label is numeric (if categorical, convert)\n",
    "    if df[label_col].dtype == 'object' or not np.issubdtype(df[label_col].dtype, np.number):\n",
    "        df['label'] = pd.Categorical(df[label_col]).codes\n",
    "    else:\n",
    "        df['label'] = df[label_col].astype(int)\n",
    "\n",
    "    return df[['text', 'label'] + [c for c in df.columns if c.startswith('clean_')]]\n",
    "\n",
    "# ---------- 3) Tokenizer fitting & sequence creation ----------\n",
    "def build_and_apply_tokenizer(texts,\n",
    "                              vocab_size=None,\n",
    "                              oov_token=\"<OOV>\",\n",
    "                              tokenizer_save_path=None,\n",
    "                              filters=''):\n",
    "    \"\"\"\n",
    "    - texts: iterable of text strings (fit on training texts ONLY)\n",
    "    - vocab_size: if None, we let Tokenizer keep all words, then we will cap indices at desired vocab_size when building embedding later.\n",
    "    - oov_token: token for out-of-vocab words\n",
    "    - tokenizer_save_path: if provided, the tokenizer json will be saved there\n",
    "    Returns tokenizer object.\n",
    "    \"\"\"\n",
    "    # If you have a very large corpus consider using num_words=vocab_size when creating Tokenizer\n",
    "    if vocab_size is None:\n",
    "        tokenizer = Tokenizer(oov_token=oov_token, filters=filters)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # Optionally save tokenizer\n",
    "    if tokenizer_save_path:\n",
    "        tokenizer_json = tokenizer.to_json()\n",
    "        Path(tokenizer_save_path).write_text(tokenizer_json, encoding='utf-8')\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_padded_sequences(tokenizer, texts, maxlen, padding='post', truncating='post', num_words=None):\n",
    "    \"\"\"\n",
    "    Convert texts -> sequences -> padded arrays.\n",
    "    num_words: if set, will cap token indices to num_words (useful to enforce vocab limit)\n",
    "    \"\"\"\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    if num_words is not None:\n",
    "        # cap indices: any index >= num_words becomes OOV (we map them to tokenizer.word_index[oov_token])\n",
    "        oov_index = tokenizer.word_index.get(tokenizer.oov_token)\n",
    "        if oov_index is None:\n",
    "            # fallback\n",
    "            oov_index = 1\n",
    "        capped_seqs = []\n",
    "        for s in seqs:\n",
    "            capped_s = [w if (w is not None and w < num_words) else oov_index for w in s]\n",
    "            capped_seqs.append(capped_s)\n",
    "        seqs = capped_seqs\n",
    "    padded = pad_sequences(seqs, maxlen=maxlen, padding=padding, truncating=truncating)\n",
    "    return padded\n",
    "\n",
    "# ---------- 4) Utility to choose maxlen from dataset ----------\n",
    "def choose_maxlen_from_texts(texts, percentile=95):\n",
    "    \"\"\"\n",
    "    Compute token counts per text and return percentile-based maxlen.\n",
    "    Use this to choose a good maxlen that covers most samples while avoiding huge padding.\n",
    "    \"\"\"\n",
    "    # quick token count approximation using whitespace split after cleaning\n",
    "    lens = [len(t.split()) for t in texts]\n",
    "    return int(np.percentile(lens, percentile)), lens\n",
    "\n",
    "# ---------- 5) Example wiring: train/val split, fit tokenizer, pad sequences ----------\n",
    "def prepare_tokenized_dataset(df,\n",
    "                              tokenizer_vocab=None,\n",
    "                              maxlen=None,\n",
    "                              test_size=0.1,\n",
    "                              random_state=42,\n",
    "                              tokenizer_save_path='tokenizer.json'):\n",
    "    \"\"\"\n",
    "    Full wiring:\n",
    "    - split into train/val\n",
    "    - fit tokenizer on train.text\n",
    "    - choose maxlen if None (95th percentile)\n",
    "    - convert train/val to padded sequences\n",
    "    Returns: tokenizer, X_train, X_val, y_train, y_val, stats dict\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df['label'])\n",
    "    # Fit tokenizer on training data only\n",
    "    tokenizer = build_and_apply_tokenizer(train_df['text'].tolist(), vocab_size=tokenizer_vocab, tokenizer_save_path=tokenizer_save_path)\n",
    "\n",
    "    # choose maxlen if not provided\n",
    "    if maxlen is None:\n",
    "        chosen_maxlen, lens = choose_maxlen_from_texts(train_df['text'].tolist(), percentile=95)\n",
    "        maxlen = chosen_maxlen\n",
    "    else:\n",
    "        lens = [len(t.split()) for t in train_df['text'].tolist()]\n",
    "\n",
    "    # convert to sequences\n",
    "    X_train = texts_to_padded_sequences(tokenizer, train_df['text'].tolist(), maxlen=maxlen, num_words=tokenizer_vocab)\n",
    "    X_val = texts_to_padded_sequences(tokenizer, val_df['text'].tolist(), maxlen=maxlen, num_words=tokenizer_vocab)\n",
    "\n",
    "    y_train = train_df['label'].values\n",
    "    y_val = val_df['label'].values\n",
    "\n",
    "    stats = {\n",
    "        'vocab_size_reported_by_tokenizer': len(tokenizer.word_index) + 1, # +1 for padding index 0\n",
    "        'chosen_maxlen': maxlen,\n",
    "        'train_text_count': len(train_df),\n",
    "        'val_text_count': len(val_df),\n",
    "        'train_length_percentile_95': int(np.percentile(lens, 95))\n",
    "    }\n",
    "\n",
    "    return tokenizer, X_train, X_val, y_train, y_val, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4416bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "                                     Concatenate, Dense, Dropout, Bidirectional, LSTM)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_cnn_lstm_singleinput(vocab_size, embedding_dim, maxlen,\n",
    "                               cnn_filters=128, kernel_sizes=(3,4,5),\n",
    "                               lstm_units=128, dropout=0.3, num_classes=1):\n",
    "    \"\"\"\n",
    "    Single text input model. Use binary sigmoid for num_classes==1 else softmax for >1.\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    text_in = Input(shape=(maxlen,), name='text_input')\n",
    "\n",
    "    # Embedding (index 0 reserved for padding)\n",
    "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, name='embedding')(text_in)\n",
    "\n",
    "    # CNN towers for multiple kernel sizes\n",
    "    conv_pools = []\n",
    "    for k in kernel_sizes:\n",
    "        c = Conv1D(filters=cnn_filters, kernel_size=k, activation='relu', padding='valid')(emb)\n",
    "        p = GlobalMaxPooling1D()(c)\n",
    "        conv_pools.append(p)\n",
    "    cnn_concat = Concatenate()(conv_pools) if len(conv_pools) > 1 else conv_pools[0]\n",
    "\n",
    "    # Optional dense before LSTM (here we pass CNN features to LSTM as context — another approach is CNN -> pooling -> dense -> classify)\n",
    "    # Instead we'll use CNN features + a BiLSTM operating on embeddings\n",
    "    blstm = Bidirectional(LSTM(lstm_units, return_sequences=False))(emb)  # run LSTM on embeddings (not on CNN output)\n",
    "    merged = Concatenate()([cnn_concat, blstm])\n",
    "\n",
    "    x = Dropout(dropout)(merged)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        out = Dense(1, activation='sigmoid', name='output')(x)\n",
    "        loss = 'binary_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "    else:\n",
    "        out = Dense(num_classes, activation='softmax', name='output')(x)\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "        metrics = ['sparse_categorical_accuracy']\n",
    "\n",
    "    model = Model(inputs=[text_in], outputs=[out])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss=loss, metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "77c0a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df_optionA(df,\n",
    "                          subject_col='subject',\n",
    "                          body_col='body',\n",
    "                          urls_col='urls',\n",
    "                          label_col='label'):\n",
    "    \"\"\"\n",
    "    Preprocess for Option A:\n",
    "    Combine subject+body; if urls==1, append <HAS_URL> token.\n",
    "    \"\"\"\n",
    "    df = train_df.copy()\n",
    "    df['clean_subject'] = df[subject_col].apply(clean_text)\n",
    "    df['clean_body'] = df[body_col].apply(clean_text)\n",
    "\n",
    "    # combine subject + body\n",
    "    df['text'] = \"<SUBJ> \" + df['clean_subject'] + \" <BODY> \" + df['clean_body']\n",
    "\n",
    "    # append the <HAS_URL> token where urls==1\n",
    "    if urls_col in df.columns:\n",
    "        df['text'] = np.where(df[urls_col] == 1, df['text'] + \" <HAS_URL>\", df['text'])\n",
    "\n",
    "    # numeric labels\n",
    "    if df[label_col].dtype == 'object' or not np.issubdtype(df[label_col].dtype, np.number):\n",
    "        df['label'] = pd.Categorical(df[label_col]).codes\n",
    "    else:\n",
    "        df['label'] = df[label_col].astype(int)\n",
    "\n",
    "    return df[['text', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "00f5e919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44701</th>\n",
       "      <td>[THE BULL REPORT! !! Butt sym: CHVCCurrent: $ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111842</th>\n",
       "      <td>Confidential I will send you updates ---------...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421894</th>\n",
       "      <td>\\nHealthMedsCustomerSupport\\nhttp://e6raeq.blu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65390</th>\n",
       "      <td>On mahonia margining, I spoke with mike garber...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144759</th>\n",
       "      <td>begin pgp signed message hash shaescapenumber...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150042</th>\n",
       "      <td>this is amazing stuff add some inches fast saf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76597</th>\n",
       "      <td>I am sure you all will be reviewing the approp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357858</th>\n",
       "      <td>----------------------  Darron C Giron/HOU/ECT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401535</th>\n",
       "      <td>carlos ,\\ni created the following deal for 4 /...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312671</th>\n",
       "      <td>are you ready ?\\nwe are accepting mortgage req...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322798 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "44701   [THE BULL REPORT! !! Butt sym: CHVCCurrent: $ ...      1\n",
       "111842  Confidential I will send you updates ---------...      0\n",
       "421894  \\nHealthMedsCustomerSupport\\nhttp://e6raeq.blu...      1\n",
       "65390   On mahonia margining, I spoke with mike garber...      0\n",
       "144759   begin pgp signed message hash shaescapenumber...      0\n",
       "...                                                   ...    ...\n",
       "150042  this is amazing stuff add some inches fast saf...      1\n",
       "76597   I am sure you all will be reviewing the approp...      0\n",
       "357858  ----------------------  Darron C Giron/HOU/ECT...      0\n",
       "401535  carlos ,\\ni created the following deal for 4 /...      0\n",
       "312671  are you ready ?\\nwe are accepting mortgage req...      1\n",
       "\n",
       "[322798 rows x 2 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f139737e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58a085e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ebacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6869b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin = pd.read_csv(r\"C:\\Users\\LENOVO\\Documents\\B TECH SEM 5\\ProjectSEC\\clean_spam_email_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c912ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Author: allison\\nDate: Fri Jun 29 10:33:22 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I expect to be here. 713 276 7374. Maybe a win...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on mon escapenumber escapenumber escapenumber ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['free play whether better off for some of str...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks for the reply. Please see embedded resp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555220</th>\n",
       "      <td>It was so nice to hear from you. I am glad all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555221</th>\n",
       "      <td>-----  Richard B Sanders/HOU/ECT on 12/01/2000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555222</th>\n",
       "      <td>Louise/John: These changes are consistent with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555223</th>\n",
       "      <td>['Anatrim - the most effective flesh loss blen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555224</th>\n",
       "      <td>Yes Tana I do indeed remember meeting you. Let...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0       Author: allison\\nDate: Fri Jun 29 10:33:22 200...      0\n",
       "1       I expect to be here. 713 276 7374. Maybe a win...      0\n",
       "2       on mon escapenumber escapenumber escapenumber ...      0\n",
       "3       ['free play whether better off for some of str...      1\n",
       "4       Thanks for the reply. Please see embedded resp...      0\n",
       "...                                                   ...    ...\n",
       "555220  It was so nice to hear from you. I am glad all...      0\n",
       "555221  -----  Richard B Sanders/HOU/ECT on 12/01/2000...      0\n",
       "555222  Louise/John: These changes are consistent with...      0\n",
       "555223  ['Anatrim - the most effective flesh loss blen...      1\n",
       "555224  Yes Tana I do indeed remember meeting you. Let...      0\n",
       "\n",
       "[555225 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b06852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    331327\n",
       "1    223898\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c46364e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(42756)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd436928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Author: allison\\nDate: Fri Jun 29 10:33:22 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I expect to be here. 713 276 7374. Maybe a win...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on mon escapenumber escapenumber escapenumber ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['free play whether better off for some of str...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks for the reply. Please see embedded resp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555220</th>\n",
       "      <td>It was so nice to hear from you. I am glad all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555221</th>\n",
       "      <td>-----  Richard B Sanders/HOU/ECT on 12/01/2000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555222</th>\n",
       "      <td>Louise/John: These changes are consistent with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555223</th>\n",
       "      <td>['Anatrim - the most effective flesh loss blen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555224</th>\n",
       "      <td>Yes Tana I do indeed remember meeting you. Let...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0       Author: allison\\nDate: Fri Jun 29 10:33:22 200...      0\n",
       "1       I expect to be here. 713 276 7374. Maybe a win...      0\n",
       "2       on mon escapenumber escapenumber escapenumber ...      0\n",
       "3       ['free play whether better off for some of str...      1\n",
       "4       Thanks for the reply. Please see embedded resp...      0\n",
       "...                                                   ...    ...\n",
       "555220  It was so nice to hear from you. I am glad all...      0\n",
       "555221  -----  Richard B Sanders/HOU/ECT on 12/01/2000...      0\n",
       "555222  Louise/John: These changes are consistent with...      0\n",
       "555223  ['Anatrim - the most effective flesh loss blen...      1\n",
       "555224  Yes Tana I do indeed remember meeting you. Let...      0\n",
       "\n",
       "[512469 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77dbc2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Author: allison\\nDate: Fri Jun 29 10:33:22 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I expect to be here. 713 276 7374. Maybe a win...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on mon escapenumber escapenumber escapenumber ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['free play whether better off for some of str...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks for the reply. Please see embedded resp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555220</th>\n",
       "      <td>It was so nice to hear from you. I am glad all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555221</th>\n",
       "      <td>-----  Richard B Sanders/HOU/ECT on 12/01/2000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555222</th>\n",
       "      <td>Louise/John: These changes are consistent with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555223</th>\n",
       "      <td>['Anatrim - the most effective flesh loss blen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555224</th>\n",
       "      <td>Yes Tana I do indeed remember meeting you. Let...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message  label\n",
       "0       Author: allison\\nDate: Fri Jun 29 10:33:22 200...      0\n",
       "1       I expect to be here. 713 276 7374. Maybe a win...      0\n",
       "2       on mon escapenumber escapenumber escapenumber ...      0\n",
       "3       ['free play whether better off for some of str...      1\n",
       "4       Thanks for the reply. Please see embedded resp...      0\n",
       "...                                                   ...    ...\n",
       "555220  It was so nice to hear from you. I am glad all...      0\n",
       "555221  -----  Richard B Sanders/HOU/ECT on 12/01/2000...      0\n",
       "555222  Louise/John: These changes are consistent with...      0\n",
       "555223  ['Anatrim - the most effective flesh loss blen...      1\n",
       "555224  Yes Tana I do indeed remember meeting you. Let...      0\n",
       "\n",
       "[555225 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9373ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
